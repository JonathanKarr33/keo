{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEL\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_task = 'nel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed gold standard data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tool_results/spacy_entity_linker/spacy_entitylinker_sm.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ../tool_results/spacy_entity_linker/spacy_entitylinker_sm.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sample</th>\n",
       "      <th>entity</th>\n",
       "      <th>qid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19990213001379A</td>\n",
       "      <td>ACFT WAS TAXIING FOR TAKE OFF WHEN IT LOST CON...</td>\n",
       "      <td>['ACFT', None, None]</td>\n",
       "      <td>['Q11436', None, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19800217031649I</td>\n",
       "      <td>AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...</td>\n",
       "      <td>['TAKEOFF', None, None]</td>\n",
       "      <td>['Q854248', None, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19800217031649I</td>\n",
       "      <td>AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...</td>\n",
       "      <td>['ENGINE', None, None]</td>\n",
       "      <td>['Q743004', None, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19800217031649I</td>\n",
       "      <td>AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...</td>\n",
       "      <td>['WING', None, None]</td>\n",
       "      <td>['Q161358', None, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19800217031649I</td>\n",
       "      <td>AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...</td>\n",
       "      <td>['FUEL TANK', 'TANK', None]</td>\n",
       "      <td>['Q1411232', 'Q1047832', None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>20030620012809I</td>\n",
       "      <td>(-23) PILOT FAILED TO ASSURE THE OIL FILLER CA...</td>\n",
       "      <td>['PILOT', None, None]</td>\n",
       "      <td>['Q2095549', None, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>20030620012809I</td>\n",
       "      <td>(-23) PILOT FAILED TO ASSURE THE OIL FILLER CA...</td>\n",
       "      <td>['OIL FILLER CAP', 'CAP', None]</td>\n",
       "      <td>[None, 'Q2488579', None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>20030620012809I</td>\n",
       "      <td>(-23) PILOT FAILED TO ASSURE THE OIL FILLER CA...</td>\n",
       "      <td>['OIL', None, None]</td>\n",
       "      <td>['Q42962', None, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>20030620012809I</td>\n",
       "      <td>(-23) PILOT FAILED TO ASSURE THE OIL FILLER CA...</td>\n",
       "      <td>['OIL', None, None]</td>\n",
       "      <td>['Q42962', None, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>20030620012809I</td>\n",
       "      <td>(-23) PILOT FAILED TO ASSURE THE OIL FILLER CA...</td>\n",
       "      <td>['ENGINE', None, None]</td>\n",
       "      <td>['Q743004', None, None]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                             sample  \\\n",
       "0    19990213001379A  ACFT WAS TAXIING FOR TAKE OFF WHEN IT LOST CON...   \n",
       "1    19800217031649I  AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...   \n",
       "2    19800217031649I  AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...   \n",
       "3    19800217031649I  AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...   \n",
       "4    19800217031649I  AFTER TAKEOFF, ENGINE QUIT. WING FUEL TANK SUM...   \n",
       "..               ...                                                ...   \n",
       "505  20030620012809I  (-23) PILOT FAILED TO ASSURE THE OIL FILLER CA...   \n",
       "506  20030620012809I  (-23) PILOT FAILED TO ASSURE THE OIL FILLER CA...   \n",
       "507  20030620012809I  (-23) PILOT FAILED TO ASSURE THE OIL FILLER CA...   \n",
       "508  20030620012809I  (-23) PILOT FAILED TO ASSURE THE OIL FILLER CA...   \n",
       "509  20030620012809I  (-23) PILOT FAILED TO ASSURE THE OIL FILLER CA...   \n",
       "\n",
       "                              entity                             qid  \n",
       "0               ['ACFT', None, None]          ['Q11436', None, None]  \n",
       "1            ['TAKEOFF', None, None]         ['Q854248', None, None]  \n",
       "2             ['ENGINE', None, None]         ['Q743004', None, None]  \n",
       "3               ['WING', None, None]         ['Q161358', None, None]  \n",
       "4        ['FUEL TANK', 'TANK', None]  ['Q1411232', 'Q1047832', None]  \n",
       "..                               ...                             ...  \n",
       "505            ['PILOT', None, None]        ['Q2095549', None, None]  \n",
       "506  ['OIL FILLER CAP', 'CAP', None]        [None, 'Q2488579', None]  \n",
       "507              ['OIL', None, None]          ['Q42962', None, None]  \n",
       "508              ['OIL', None, None]          ['Q42962', None, None]  \n",
       "509           ['ENGINE', None, None]         ['Q743004', None, None]  \n",
       "\n",
       "[510 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# path to the gold standard file\n",
    "task_gold_standard_path = f\"../gold_standard/processed/{nlp_task}.csv\"\n",
    "\n",
    "# load processed NER Gold Standard Data\n",
    "gs = pd.read_csv(task_gold_standard_path)\n",
    "\n",
    "gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed outputs tools to be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(nlp_task, tool):\n",
    "    # load \n",
    "    tool_path = f\"../tool_results/PROCESSED/{nlp_task}/\"\n",
    "    df = pd.read_csv(tool_path+tool+'.csv')\n",
    "    # standardize\n",
    "    #df = (df.reset_index(drop=True)).rename(columns={'c5_unique_id': 'id', 'c119_text': 'sample'}).drop(columns=['index'])\n",
    "    return df\n",
    "\n",
    "# load output from each tool to be evaluated\n",
    "tools = [\"spacy\"] # Note that should exist files for each tool at ~/tool_results/nel//\n",
    "df_tools = { tool: load_df(nlp_task, tool)   for tool in tools }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some samples for each tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sample</th>\n",
       "      <th>qid</th>\n",
       "      <th>entity</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19760606015529A</td>\n",
       "      <td>SUFFICIENT OPPORTUNITY EXISTED TO RELEASE WHEN...</td>\n",
       "      <td>193538</td>\n",
       "      <td>Opportunity</td>\n",
       "      <td>NASA Mars rover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19760606015529A</td>\n",
       "      <td>SUFFICIENT OPPORTUNITY EXISTED TO RELEASE WHEN...</td>\n",
       "      <td>3785514</td>\n",
       "      <td>High</td>\n",
       "      <td>song by Lighthouse Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19780111000459A</td>\n",
       "      <td>ACFT DISPATCHER HARRASSMENT OF PILOT. PILOT FO...</td>\n",
       "      <td>67935434</td>\n",
       "      <td>United States Army Combat Fitness Test</td>\n",
       "      <td>Physical fitness test for the United States Army</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19780111000459A</td>\n",
       "      <td>ACFT DISPATCHER HARRASSMENT OF PILOT. PILOT FO...</td>\n",
       "      <td>2044212</td>\n",
       "      <td>PILOT</td>\n",
       "      <td>historic programming language</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                             sample  \\\n",
       "0  19760606015529A  SUFFICIENT OPPORTUNITY EXISTED TO RELEASE WHEN...   \n",
       "1  19760606015529A  SUFFICIENT OPPORTUNITY EXISTED TO RELEASE WHEN...   \n",
       "2  19780111000459A  ACFT DISPATCHER HARRASSMENT OF PILOT. PILOT FO...   \n",
       "3  19780111000459A  ACFT DISPATCHER HARRASSMENT OF PILOT. PILOT FO...   \n",
       "\n",
       "        qid                                  entity  \\\n",
       "0    193538                             Opportunity   \n",
       "1   3785514                                    High   \n",
       "2  67935434  United States Army Combat Fitness Test   \n",
       "3   2044212                                   PILOT   \n",
       "\n",
       "                                        description  \n",
       "0                                   NASA Mars rover  \n",
       "1                         song by Lighthouse Family  \n",
       "2  Physical fitness test for the United States Army  \n",
       "3                     historic programming language  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tool in tools:\n",
    "    print(tool)\n",
    "    display(df_tools[tool].head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def qid_semantic_similarity(q1, q2, similarity_type):\n",
    "    # sim_api_call('Q1875633', 'Q42501', \"class\")\n",
    "    api_url = f\"https://kgtk.isi.edu/similarity_api?q1=Q{q1}&q2=Q{q2}&similarity_type={similarity_type}\"\n",
    "    #print(api_url)\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        # Check if \"error\" key is in the response\n",
    "        if \"error\" in response_json:\n",
    "            # Return this when the \"error\" key is present\n",
    "            return {\"similarity\": -1, \"q1\": q1, \"q2\": q2}\n",
    "        else:\n",
    "            return response_json\n",
    "    else:\n",
    "        return {\"similarity\": -1, \"q1\": q1, \"q2\": q2}\n",
    "    \n",
    "def is_similar_entity(label1, label2):\n",
    "    \"\"\"\n",
    "    Check if any word in label1 is in label2 and vice-versa.\n",
    "\n",
    "    Parameters:\n",
    "    - label1: The first entity label as a string.\n",
    "    - label2: The second entity label as a string.\n",
    "\n",
    "    Returns:\n",
    "    - True if any word in label1 is in label2 and vice-versa, otherwise False.\n",
    "    \"\"\"\n",
    "    # Tokenize labels into sets of words\n",
    "    words_label1 = set(label1.lower().split())\n",
    "    words_label2 = set(label2.lower().split())\n",
    "    \n",
    "    # Check for intersection between sets\n",
    "    common_words = words_label1.intersection(words_label2)\n",
    "\n",
    "    return bool(common_words)  # Returns True if there is any common word, False otherwise\n",
    "\n",
    "\n",
    "def pairing_of_entities(gs, tools_df):\n",
    "    results = []\n",
    "\n",
    "    for _, gs_row in gs.iterrows():\n",
    "        for _, tool_row in tools_df.iterrows():\n",
    "            if gs_row['id'] == tool_row['id']:  # Match IDs before comparing\n",
    "                \n",
    "                # entity lexical similarity flag\n",
    "                entity_sim = is_similar_entity(gs_row['entity'], tool_row['entity'])\n",
    "                \n",
    "                # qid semantic similarity scores\n",
    "                qid_sim_jc_response = qid_semantic_similarity(gs_row['qid'], tool_row['qid'], \"jc\")\n",
    "                qid_sim_class_response = qid_semantic_similarity(gs_row['qid'], tool_row['qid'], \"class\")\n",
    "                \n",
    "                results.append({\n",
    "                    'id': gs_row['id'],\n",
    "                    'entity_gs': gs_row['entity'],\n",
    "                    'qid_gs': gs_row['qid'],\n",
    "                    'entity_tool': tool_row['entity'],\n",
    "                    'qid_tool': tool_row['qid'],\n",
    "                    'entity_sim': entity_sim,\n",
    "                    'qid_sim_jc': None, # qid_sim_jc_response['similarity'],\n",
    "                    'qid_sim_class': None #qid_sim_class_response['similarity']\n",
    "                })\n",
    "                \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def evaluate_entity_similarity(gs, tools_df):\n",
    "    print(\"Evaluating entity similarity...\")\n",
    "    \n",
    "def pairing_of_entities_with_sim(gs, tools_df, output_file='results.csv'):\n",
    "    results = []\n",
    "    write_header = not os.path.exists(output_file)  # Check if the file exists to decide on writing headers\n",
    "    \n",
    "    for _, gs_row in gs.iterrows():\n",
    "        for _, tool_row in tools_df.iterrows():\n",
    "            if gs_row['id'] == tool_row['id']:  # Match IDs before comparing\n",
    "                \n",
    "                # entity lexical similarity flag\n",
    "                entity_sim = is_similar_entity(gs_row['entity'], tool_row['entity'])\n",
    "                \n",
    "                # qid semantic similarity scores\n",
    "                qid_sim_jc_response = qid_semantic_similarity(gs_row['qid'], tool_row['qid'], \"jc\")\n",
    "                qid_sim_class_response = qid_semantic_similarity(gs_row['qid'], tool_row['qid'], \"class\")\n",
    "                \n",
    "                # Prepare the row to be written\n",
    "                row_to_write = {\n",
    "                    'id': gs_row['id'],\n",
    "                    'entity_gs': gs_row['entity'],\n",
    "                    'qid_gs': gs_row['qid'],\n",
    "                    'entity_tool': tool_row['entity'],\n",
    "                    'qid_tool': tool_row['qid'],\n",
    "                    'entity_sim': entity_sim,\n",
    "                    'qid_sim_jc': qid_sim_jc_response['similarity'],  # Assume these functions return similarity directly\n",
    "                    'qid_sim_class': qid_sim_class_response['similarity']\n",
    "                }\n",
    "                \n",
    "                # Append row to the results list for DataFrame conversion\n",
    "                results.append(row_to_write)\n",
    "                \n",
    "                # Convert the row to a DataFrame to use to_csv for appending\n",
    "                pd.DataFrame([row_to_write]).to_csv(output_file, mode='a', header=write_header, index=False)\n",
    "                \n",
    "                # Ensure the header is not written again\n",
    "                write_header = False\n",
    "                \n",
    "    # Convert all accumulated results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "import os, json\n",
    "\n",
    "def call_semantic_similarity(input_file, url):\n",
    "    file_name = os.path.basename(input_file)\n",
    "    files = {\n",
    "        'file': (file_name, open(input_file, mode='rb'), 'application/octet-stream')\n",
    "    }\n",
    "    \n",
    "    print(files)\n",
    "    resp = requests.post(url, files=files, params={'similarity_types': \"[class, jc]\"})\n",
    "    s = json.loads(resp.json())\n",
    "    return pd.DataFrame(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pairing of entity links between the gold standard and a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file = \"../evaluations/quantitative/nel/spacy_pairing.csv\"\n",
    "# pairing = pairing_of_entities_with_sim(gs, df_tools['spacy'], output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check a pairing for a specific sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pick a sample to display\n",
    "# sample_id = '19800217031649I'\n",
    "# sample_text = gs[gs.id == sample_id]['sample'].values[0]\n",
    "# pairing_to_sample = pairing[pairing.id == sample_id]\n",
    "\n",
    "# # print results for a sample\n",
    "# print(f\"Results for sample ID: {sample_id}\")\n",
    "# print(f\"Text: {sample_text}\")\n",
    "# display(pairing_to_sample)\n",
    "\n",
    "# # print only rows where they have same entity name \n",
    "# print(f\"Show only results for same entity names for sample ID: {sample_id}\")\n",
    "# results_same_qid = pairing_to_sample[['id','entity_gs','entity_tool', 'qid_gs','qid_tool' ]][pairing_to_sample.entity_sim]\n",
    "# display(results_same_qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6996778267508654\n",
      "0.7331227847702766\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#TAKEOFF\t854248\t\n",
    "#PREFLIGHT\t2108354\t\n",
    "# Q11436\n",
    "# Q11436\n",
    "# Q832489\n",
    "print(qid_semantic_similarity('11436', '320599', \"jc\")['similarity'])\n",
    "print(qid_semantic_similarity('11436', '832489', \"jc\")['similarity'])\n",
    "print(qid_semantic_similarity('216197', '216197', \"jc\")['similarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pairing with the same `Qid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same_ids = pairing[pairing.qid_tool == pairing.qid_gs]\n",
    "# print(f\"total: {same_ids.shape[0]}\")\n",
    "# display(same_ids)\n",
    "# # save to file \n",
    "# pairing_to_sample.to_csv(f\"../evaluations/quantitative/{nlp_task}/{tool}_same_qid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIM_API = 'https://kgtk.isi.edu/similarity_api'\n",
    "# df = call_semantic_similarity('test_file.csv', SIM_API)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Continue from here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate precision and recall where `gs` is considered the ground truth and `tool` is the answers provided by some tool or method, we first need to define these metrics in the context of your entity similarity task:\n",
    "\n",
    "- **Precision**: Of all the entities identified by `tool`, how many were correctly identified as per `gs`? This is calculated as the number of true positives (TP) divided by the number of true positives and false positives (TP + FP).\n",
    "\n",
    "- **Recall**: Of all the relevant entities present in `gs`, how many were identified by `tool`? This is calculated as the number of true positives (TP) divided by the number of true positives and false negatives (TP + FN).\n",
    "\n",
    "Here's a step-by-step approach:\n",
    "\n",
    "1. **True Positives (TP)**: Entities in `tool` that match entities in `gs` for the same `id` (check `is_similar_entity` function to understand the match criteria).\n",
    "2. **False Positives (FP)**: Entities in `tool` that do not match any entity in `gs` for the same `id`.\n",
    "3. **False Negatives (FN)**: Entities in `gs` that do not match any entity in `tool` for the same `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similar_entity(gs_entity, tool_entities):\n",
    "    \"\"\"\n",
    "    Check if the gs_entity is similar to any of the entities in the tool_entities list.\n",
    "    An entity is considered similar if it's a substring of any entity in the list, or vice versa.\n",
    "    \n",
    "    Parameters:\n",
    "    - gs_entity: The entity from the gs DataFrame.\n",
    "    - tool_entities: A list of entities from the df_tool DataFrame for a given id.\n",
    "    \n",
    "    Returns:\n",
    "    - True if similar entity is found, False otherwise.\n",
    "    \"\"\"\n",
    "    for tool_entity in tool_entities:\n",
    "        if gs_entity in tool_entity or tool_entity in gs_entity:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_entity_similarity(gs, df_tool):\n",
    "    \"\"\"\n",
    "    For each row in the gs DataFrame, check if the entity is similar to any entity in the df_tool,\n",
    "    considering only rows with the same id.\n",
    "    \n",
    "    Parameters:\n",
    "    - gs: The ground truth DataFrame with columns ['id', 'sample', 'entities'].\n",
    "    - df_tool: The tool DataFrame with columns ['id', 'sample', 'entities', 'POS tags', 'labels'].\n",
    "    \n",
    "    Returns:\n",
    "    - The gs DataFrame with an additional 'Similarity' column indicating if a similar entity was found in df_tool.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store similarity results\n",
    "    similarities = []\n",
    "    \n",
    "    # Iterate through each row in gs\n",
    "    for index, row in gs.iterrows():\n",
    "        # Extract the id and entity for the current row\n",
    "        gs_id, gs_entity = row['id'], row['entities']\n",
    "        \n",
    "        # Find entities in df_tool with the same id\n",
    "        tool_entities = df_tool.loc[df_tool['id'] == gs_id, 'entities'].tolist()\n",
    "        \n",
    "        # Check for similarity and append the result\n",
    "        similarities.append(is_similar_entity(gs_entity, tool_entities))\n",
    "        \n",
    "    # Add the similarity results to the gs DataFrame\n",
    "    gs['Similarity'] = similarities\n",
    "    \n",
    "    return gs\n",
    "\n",
    "\n",
    "def calculate_precision_recall_f1(gs, df_tool):\n",
    "    \"\"\"\n",
    "    Calculate precision and recall based on entities comparison between gs (ground truth) and df_tool (answers).\n",
    "    \n",
    "    Parameters:\n",
    "    - gs: DataFrame with columns ['id', 'sample', 'entities'] representing the ground truth.\n",
    "    - df_tool: DataFrame with columns ['id', 'sample', 'entities', 'POS tags', 'labels'] representing the tool's answers.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple containing precision and recall.\n",
    "    \"\"\"\n",
    "    TP = 0  # True Positives\n",
    "    FP = 0  # False Positives\n",
    "    FN = 0  # False Negatives\n",
    "    \n",
    "    # Check for True Positives and False Negatives by iterating over gs\n",
    "    for index, gs_row in gs.iterrows():\n",
    "        gs_id, gs_entity = gs_row['id'], gs_row['entities']\n",
    "        tool_entities = df_tool.loc[df_tool['id'] == gs_id, 'entities'].tolist()\n",
    "        \n",
    "        if any(gs_entity in tool_entity or tool_entity in gs_entity for tool_entity in tool_entities):\n",
    "            TP += 1\n",
    "        else:\n",
    "            FN += 1\n",
    "    \n",
    "    # Check for False Positives by iterating over df_tool\n",
    "    for index, tool_row in df_tool.iterrows():\n",
    "        tool_id, tool_entity = tool_row['id'], tool_row['entities']\n",
    "        gs_entities = gs.loc[gs['id'] == tool_id, 'entities'].tolist()\n",
    "        \n",
    "        if not any(tool_entity in gs_entity or gs_entity in tool_entity for gs_entity in gs_entities):\n",
    "            FP += 1\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    \n",
    "    # Calculating the F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "\n",
    "def evaluate_nlr(gs, df_tools, tools):\n",
    "    \"\"\"\n",
    "    Evaluate NLR tools by calculating precision, recall, and F1 score, and return a sorted and rounded DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - gs: The ground truth DataFrame with columns ['id', 'sample', 'entities'].\n",
    "    - df_tools: A dictionary with DataFrames for each tool, where each DataFrame contains ['id', 'sample', 'entities', ...].\n",
    "    - tools: A list of tool names corresponding to keys in df_tools.\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame with each tool's precision, recall, and F1 score, sorted by F1 score in descending order and rounded to 2 decimal places.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for tool in tools:\n",
    "        precision, recall, f1_score = calculate_precision_recall_f1(gs, df_tools[tool])  # Assume this function is defined\n",
    "        results.append({\n",
    "            'Tool': tool,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1_score\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df_sorted = results_df.sort_values(by='F1 Score', ascending=False)\n",
    "    \n",
    "    return results_df_sorted.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# results_df_sorted = evaluate_nlr(gs, df_tools, tools)\n",
    "# results_df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some matches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_entity_similarity(gs, df_tools['spacy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
