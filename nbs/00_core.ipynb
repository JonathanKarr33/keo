{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Functions\n",
    "\n",
    "> This page defines all boilerplate functions for analyzing and evaluating the NLP tools in the 2K paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikidata Queries\n",
    "\n",
    "\n",
    "The functions interact with Wikidata by sending SPARQL queries to the Wikidata Query Service (WDQS), and the goal is to retrieve information about items or properties based on specific search criteria (e.g., matching a search term in labels, descriptions, or aliases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "\n",
    "async def _fetch_query(session: str, # The aiohttp client session.\n",
    "                        query: str  # The SPARQL query string.\n",
    "                        ) -> str:   # The JSON response from the Wikidata SPARQL endpoint.\n",
    "    \n",
    "    \"\"\"\n",
    "    Asynchronously fetch the query result.\n",
    "    \"\"\"\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0', 'Accept': 'application/sparql-results+json'}\n",
    "    async with session.get(url, headers=headers, params={'query': query, 'format': 'json'}) as response:\n",
    "        if response.status == 200:\n",
    "            return await response.json()\n",
    "        return None\n",
    "\n",
    "async def _search_wikidata_properties_async(search_term: str    # The term to search for in property names and aliases.\n",
    "                                            ) -> pd.DataFrame:  # A DataFrame of found properties with their ID, label, description, and aliases, sorted by Property ID.\n",
    "    \"\"\"\n",
    "    Asynchronously search Wikidata properties by name or aliases and return a sorted pandas DataFrame.\n",
    "    \"\"\"\n",
    "    query = f'''\n",
    "    SELECT DISTINCT ?property ?propertyLabel ?propertyDescription (GROUP_CONCAT(DISTINCT ?alias;separator=\", \") AS ?aliases)\n",
    "    WHERE {{\n",
    "      ?property a wikibase:Property .\n",
    "      {{?property rdfs:label ?label FILTER (LANG(?label) = \"en\").}}\n",
    "      OPTIONAL {{?property skos:altLabel ?alias FILTER (LANG(?alias) = \"en\").}}\n",
    "      FILTER(CONTAINS(LCASE(?label), LCASE(\"{search_term}\")) || CONTAINS(LCASE(?alias), LCASE(\"{search_term}\"))).\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }}\n",
    "    GROUP BY ?property ?propertyLabel ?propertyDescription\n",
    "    LIMIT 5\n",
    "    '''\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        json_response = await _fetch_query(session, query)\n",
    "        if json_response:\n",
    "            results = json_response.get('results', {}).get('bindings', [])\n",
    "            data = [{\n",
    "                \"Property ID\": result['property']['value'].split('/')[-1][1:],\n",
    "                \"Label\": result['propertyLabel']['value'],\n",
    "                \"Description\": result['propertyDescription']['value'],\n",
    "                \"Aliases\": result.get('aliases', {}).get('value', 'N/A')\n",
    "            } for result in results]\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            df['Property ID'] = pd.to_numeric(df['Property ID'], errors='coerce')\n",
    "            df = df.sort_values(by='Property ID').reset_index(drop=True)\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Failed to retrieve data\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# Running the asynchronous function\n",
    "def search_wikidata_properties(search_term: str # A string containing the term to search for in the property names and aliases.\n",
    "                            ) -> pd.DataFrame:  # A DataFrame of found properties with their ID, label, description, and aliases, sorted by Property ID.\n",
    "    \"\"\"\n",
    "    Main function to asynchronously search Wikidata properties by name or aliases and return a sorted pandas DataFrame.\n",
    "    \"\"\"\n",
    "    nest_asyncio.apply()\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(_search_wikidata_properties_async(search_term))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of using `search wikidata properties` function, where the search term is \"**used for**\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_props = search_wikidata_properties(\"used for\")\n",
    "df_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "\n",
    "def excel_to_csv_to_df(excel_path, csv_path):\n",
    "    \"\"\"\n",
    "    Reads an Excel file, writes its contents to a CSV file, and then reads the CSV\n",
    "    file back into a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - excel_path: str, path to the source Excel file.\n",
    "    - csv_path: str, path where the CSV file will be saved.\n",
    "    \n",
    "    Returns:\n",
    "    - df: pandas.DataFrame, the content of the CSV file as a DataFrame.\n",
    "    \"\"\"\n",
    "    # Read and store content of an excel file\n",
    "    read_file = pd.read_excel(excel_path)\n",
    "    \n",
    "    # Write the dataframe object into csv file\n",
    "    read_file.to_csv(csv_path, index=None, header=True)\n",
    "    \n",
    "    # Read csv file and convert into a dataframe object\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Return the dataframe\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = excel_to_csv_to_df(\"../evaluations/qualitative/unirel_eval.xlsx\", \"../evaluations/qualitative/unirel_eval.csv\")\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
