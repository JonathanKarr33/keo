# DeepStruct Setup

### GitHub Repository
Explore the full setup process on our [GitHub repository](https://github.com/wang-research-lab/deepstruct).

### Overview

While DeepStruct provides a pretrained model, additional steps are required to prepare it for inference on unseen datasets. First, align your dataset's schema with the schema DeepStruct was trained on to ensure compatibility with the model’s recognized entity and relation types. Additionally, configuration files may need adjustments to properly format the input data.

DeepStruct schemas are based on datasets like NYT, CoNLL-04, ADE, and ACE-2005. For our evaluation, we used the NYT schema, which yielded the highest Relation Extraction (RE) F1 score of 84.6. This choice also ensures consistency and fairness when comparing DeepStruct with other models like UniRel.

### Running DeepStruct Inference

To run DeepStruct inference on your data, follow the steps below.

#### 1. Clone the Repository

```bash
mkdir deepstruct
git clone --recursive git@github.com:cgraywang/deepstruct.git
cd deepstruct
```

#### 2. Set Up the Environment

Ensure Docker is installed on your machine. Then, run the Docker container:

```bash
docker run --net=host --privileged --pid=host --gpus "device=1" --rm -it --ipc=host -v ./deepstruct:/workspace/deepstruct/ zxdu20/glm-cuda112
```

#### 3. Inside the Docker Container

Set up the environment and download the necessary models:

```bash
cd /workspace/deepstruct/deepstruct/
bash setup.sh
bash download_ckpt.sh
```

#### 4. Run DeepStruct Inference

Utilize the provided bash scripts to process NYT data and perform the RE task:

```bash
bash ./data_scripts/nyt.sh
bash ./tasks/mt/nyt_rel.sh
```

These scripts will generate a folder with files formatted according to DeepStruct’s requirements:

```
/workspace/deepstruct/data/nyt_re/
├── cached_nyt_re_train_T5TokenizerFast_256_256.pth
├── dev.json
├── schemas.json
├── test.json
├── test.jsonl.hyps
├── test.source
├── test.target
├── train.json
├── train.source
├── train.target
├── val.source
└── val.target
```

#### 5. Modify Test Files

Modify the `test.json` and `test.source` files generated in the previous step using the [prep_data.py script](https://github.com/nd-crane/trusted_ke/blob/main/re/deepstruct/prep_data.py)[^1] . To avoid errors during execution, include an empty `test.target` file. Finally, run the inference:

```bash
bash ./tasks/mt/nyt_rel.sh
```

The output file will be saved in the following location:

```
/workspace/deepstruct/data/nyt_re/test.jsonl.hyps
```

### Prepare the output data for the evaluation script
  Post-processing `test.jsonl.hyps` using the [reformat_deepstruct.ipynb](https://github.com/nd-crane/trusted_ke/blob/main/re/deepstruct/reformat_deepstruct.ipynb)[^1]   notebook. This will transform the .hyps data into a format that's compatible with our evaluation script. 
  
  
[^1]: Note that this notebook is not included in the Docker container. It resides in our repository. Therefore, we ran the script outside of Docker, copying the necessary files between the Docker environment and our project repository as needed.



----------------------------
#### Reproducibility Rating:
<img src="../../figs/star_clip.jpg" alt="Star" width="50" height="50">

The DeepStruct GitHub README is easy to follow and accurate regarding the datasets used in their benchmarks. 
For example, We confirmed their results on the NYT dataset.
Adapting their method to a custom dataset is a complex task, as the README does not provide specific instructions for this scenario.

One approach we identified is to add a new Dataset Class to their implementation, creating schemas with properties and entity types that are relevant to our data and were used in their model's training process. This also involves modifying data transformation, tokenization, and evaluation scripts.

Another approach is to use the setup of one of the datasets and tasks from their original work and adapt it to our data. The approach we chose to adopt in our project was the NYT dataset setup for the RE task. 
 


