{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a7a50-1c03-443f-a1fa-5036a7a12fcb",
   "metadata": {
    "id": "030a7a50-1c03-443f-a1fa-5036a7a12fcb"
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import typing\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# NLP and Embedding Libraries\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Graph Libraries\n",
    "import networkx as nx\n",
    "from networkx.algorithms import shortest_paths\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "openai_api_key = 'Your_Key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "NayPvat2LsMr",
   "metadata": {
    "id": "NayPvat2LsMr"
   },
   "outputs": [],
   "source": [
    "class DataPreparer:\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreparer with the CSV file path.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.raw_data = None\n",
    "        self.cleaned_data = None\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load the data from the CSV file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.raw_data = pd.read_csv(self.file_path)\n",
    "            print(\"Data loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"\n",
    "        Perform cleaning operations on the dataset:\n",
    "        - Drop unnecessary columns.\n",
    "        - Rename columns for readability.\n",
    "        - Handle missing values.\n",
    "        \"\"\"\n",
    "        if self.raw_data is None:\n",
    "            print(\"No data to clean. Please load data first.\")\n",
    "            return\n",
    "\n",
    "        self.cleaned_data = self.raw_data.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "        self.cleaned_data.rename(\n",
    "            columns={\n",
    "                'c119': 'Incident_Description',\n",
    "                'c77': 'Contributing_Factor',\n",
    "                'c79': 'Event_Context',\n",
    "                'c81': 'Role',\n",
    "                'c146': 'Weight_Category',\n",
    "                'c148': 'Aircraft_Type',\n",
    "                'c150': 'Power_Characteristics',\n",
    "                'c161': 'Outcome',\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        self.cleaned_data.fillna('Unknown', inplace=True)\n",
    "        print(\"Data cleaned successfully!\")\n",
    "\n",
    "    def normalize_text(self):\n",
    "        \"\"\"\n",
    "        Normalize text fields:\n",
    "        - Convert text to lowercase.\n",
    "        - Remove special characters.\n",
    "        \"\"\"\n",
    "        if self.cleaned_data is None:\n",
    "            print(\"No data to normalize. Please clean data first.\")\n",
    "            return\n",
    "\n",
    "        def normalize(text):\n",
    "            text = str(text).lower()  # Ensure text is a string before normalization\n",
    "            text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "            return text.strip()\n",
    "\n",
    "        for column in self.cleaned_data.columns:\n",
    "            if self.cleaned_data[column].dtype == \"object\":\n",
    "                self.cleaned_data[column] = self.cleaned_data[column].apply(normalize)\n",
    "\n",
    "        print(\"Text fields normalized successfully!\")\n",
    "\n",
    "    def get_prepared_data(self):\n",
    "        \"\"\"\n",
    "        Return the cleaned and prepared data as a Pandas DataFrame.\n",
    "        \"\"\"\n",
    "        if self.cleaned_data is None:\n",
    "            print(\"Data is not prepared yet. Please clean and normalize the data.\")\n",
    "            return None\n",
    "        return self.cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "_kzvz97a0flu",
   "metadata": {
    "id": "_kzvz97a0flu"
   },
   "outputs": [],
   "source": [
    "class DynamicGraphProcessor:\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize the graph processor with prepared data.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def extract_entities_and_relationships(self, text):\n",
    "        \"\"\"\n",
    "        Enhanced entity and relationship extraction.\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        relationships = []\n",
    "\n",
    "        # Improved entity extraction\n",
    "        entities = []\n",
    "        for ent in doc.ents:\n",
    "            if not ent.text.lower() in ['on', 'in', 'of', 'the', 'a', 'an']:  # Filter common words\n",
    "                entities.append((ent.text, ent.label_))\n",
    "\n",
    "        # Enhanced relationship extraction\n",
    "        for token in doc:\n",
    "            # Expanded dependency patterns\n",
    "            if token.dep_ in (\"nsubj\", \"dobj\", \"pobj\", \"amod\", \"compound\"):\n",
    "                if not token.is_stop and token.head.pos_ in ['VERB', 'NOUN']:  # More specific conditions\n",
    "                    subject = token.head.text\n",
    "                    obj = token.text\n",
    "                    rel = token.dep_\n",
    "\n",
    "                    # Get fuller context\n",
    "                    context = \" \".join([t.text for t in token.head.subtree\n",
    "                                     if not t.is_stop])  # Remove stopwords from context\n",
    "\n",
    "                    # Add more semantic information\n",
    "                    relationships.append({\n",
    "                        \"subject\": subject,\n",
    "                        \"relation\": rel,\n",
    "                        \"object\": obj,\n",
    "                        \"context\": context,\n",
    "                        \"verb\": token.head.lemma_ if token.head.pos_ == 'VERB' else None,\n",
    "                        \"confidence\": 1.0 if token.dep_ in (\"nsubj\", \"dobj\") else 0.8\n",
    "                    })\n",
    "\n",
    "        return relationships, entities\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        Enhanced graph building with better entity handling.\n",
    "        \"\"\"\n",
    "        for _, row in self.data.iterrows():\n",
    "            incident_description = row.get(\"Incident_Description\", \"\")\n",
    "            relationships, entities = self.extract_entities_and_relationships(incident_description)\n",
    "\n",
    "            # Add entity nodes with more context\n",
    "            for entity, entity_type in entities:\n",
    "                if len(entity.split()) <= 3:  # Filter out overly long phrases\n",
    "                    self.graph.add_node(\n",
    "                        entity,\n",
    "                        type=entity_type,\n",
    "                        category=row.get(\"Contributing_Factor\", \"Unknown\"),\n",
    "                        context=row.get(\"Event_Context\", \"\")\n",
    "                    )\n",
    "\n",
    "            # Add relationship edges with enhanced information\n",
    "            for rel in relationships:\n",
    "                subject = rel[\"subject\"]\n",
    "                obj = rel[\"object\"]\n",
    "\n",
    "                # Skip very short or common words\n",
    "                if (len(subject) <= 2 or len(obj) <= 2 or\n",
    "                    subject.lower() in ['on', 'in', 'of'] or\n",
    "                    obj.lower() in ['on', 'in', 'of']):\n",
    "                    continue\n",
    "\n",
    "                # Add nodes if they don't exist\n",
    "                for node in [subject, obj]:\n",
    "                    if node not in self.graph:\n",
    "                        self.graph.add_node(\n",
    "                            node,\n",
    "                            type=\"Entity\",\n",
    "                            category=row.get(\"Contributing_Factor\", \"Unknown\"),\n",
    "                            context=row.get(\"Event_Context\", \"\")\n",
    "                        )\n",
    "\n",
    "                # Add edge with rich metadata\n",
    "                self.graph.add_edge(\n",
    "                    subject,\n",
    "                    obj,\n",
    "                    relationship=rel[\"relation\"],\n",
    "                    context=rel[\"context\"],\n",
    "                    confidence=rel.get(\"confidence\", 1.0),\n",
    "                    incident_type=row.get(\"Contributing_Factor\", \"Unknown\")\n",
    "                )\n",
    "\n",
    "        print(f\"Graph built successfully with {self.graph.number_of_nodes()} nodes and {self.graph.number_of_edges()} edges!\")\n",
    "\n",
    "    def detect_communities(self):\n",
    "        \"\"\"\n",
    "        Detect communities in the graph using Label Propagation.\n",
    "        \"\"\"\n",
    "        from networkx.algorithms.community import asyn_lpa_communities\n",
    "\n",
    "        # Convert to undirected graph for community detection\n",
    "        undirected_graph = self.graph.to_undirected()\n",
    "        self.communities = list(asyn_lpa_communities(undirected_graph))\n",
    "\n",
    "        # Assign community labels to nodes\n",
    "        community_mapping = {}\n",
    "        for i, community in enumerate(self.communities):\n",
    "            for node in community:\n",
    "                community_mapping[node] = i\n",
    "        nx.set_node_attributes(self.graph, community_mapping, 'community')\n",
    "\n",
    "        print(f\"Detected {len(self.communities)} communities!\")\n",
    "\n",
    "    def summarize_communities(self, openai_api_key):\n",
    "        \"\"\"\n",
    "        Summarize each community using OpenAI's ChatCompletion API.\n",
    "        \"\"\"\n",
    "        def summarize_community(community_nodes):\n",
    "            \"\"\"Summarize a single community.\"\"\"\n",
    "            context = \"This is a community of entities and their relationships:\\n\"\n",
    "\n",
    "            # Add nodes and their types\n",
    "            for node in community_nodes:\n",
    "                node_type = self.graph.nodes[node].get('type', 'Unknown')\n",
    "                context += f\"- {node} (Type: {node_type})\\n\"\n",
    "\n",
    "                # Add relationships for this node\n",
    "                for neighbor in self.graph.neighbors(node):\n",
    "                    edge_data = self.graph.get_edge_data(node, neighbor)\n",
    "                    if edge_data:\n",
    "                        relationship = edge_data.get('relationship', 'related_to')\n",
    "                        context += f\"  → {relationship} → {neighbor}\\n\"\n",
    "\n",
    "            client = OpenAI(api_key=openai_api_key)\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an assistant specializing in analyzing entity relationships in aviation safety incidents.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Analyze and summarize the following entity relationships, focusing on key patterns and insights:\\n{context}\"}\n",
    "                ],\n",
    "                max_tokens=300,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "\n",
    "        self.community_summaries = {}\n",
    "        for i, community in enumerate(self.communities):\n",
    "            try:\n",
    "                summary = summarize_community(community)\n",
    "                self.community_summaries[i] = summary\n",
    "                print(f\"Community {i} Summary:\\n{summary}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error summarizing community {i}: {e}\")\n",
    "\n",
    "    def get_graph_summary(self):\n",
    "        \"\"\"\n",
    "        Provide a detailed summary of the graph structure.\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            \"Total Nodes\": self.graph.number_of_nodes(),\n",
    "            \"Total Edges\": self.graph.number_of_edges(),\n",
    "            \"Communities Detected\": len(self.communities) if hasattr(self, \"communities\") else 0,\n",
    "            \"Node Types\": self._get_node_type_distribution(),\n",
    "            \"Relationship Types\": self._get_relationship_distribution(),\n",
    "            \"Average Degree\": sum(dict(self.graph.degree()).values()) / self.graph.number_of_nodes()\n",
    "        }\n",
    "        return summary\n",
    "\n",
    "    def _get_node_type_distribution(self):\n",
    "        \"\"\"Helper method to get distribution of node types.\"\"\"\n",
    "        type_count = {}\n",
    "        for node in self.graph.nodes():\n",
    "            node_type = self.graph.nodes[node].get('type', 'Unknown')\n",
    "            type_count[node_type] = type_count.get(node_type, 0) + 1\n",
    "        return type_count\n",
    "\n",
    "    def _get_relationship_distribution(self):\n",
    "        \"\"\"Helper method to get distribution of relationship types.\"\"\"\n",
    "        rel_count = {}\n",
    "        for _, _, data in self.graph.edges(data=True):\n",
    "            rel_type = data.get('relationship', 'Unknown')\n",
    "            rel_count[rel_type] = rel_count.get(rel_type, 0) + 1\n",
    "        return rel_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Xx88G-BLMNc_",
   "metadata": {
    "id": "Xx88G-BLMNc_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def visualize_graph_with_communities(graph):\n",
    "    \"\"\"\n",
    "    Visualize the graph with nodes colored by their community.\n",
    "    \"\"\"\n",
    "    # Get community labels\n",
    "    communities = nx.get_node_attributes(graph, 'community')\n",
    "    if not communities:\n",
    "        print(\"No communities detected. Please run community detection first.\")\n",
    "        return\n",
    "\n",
    "    # Assign colors to communities\n",
    "    community_colors = {node: communities[node] for node in graph.nodes()}\n",
    "\n",
    "    # Draw the graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pos = nx.spring_layout(graph)  # Generate layout for visualization\n",
    "    nx.draw(\n",
    "        graph,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        node_color=[community_colors.get(node, 0) for node in graph.nodes()],\n",
    "        cmap=plt.cm.rainbow,\n",
    "        node_size=500,\n",
    "        font_size=8,\n",
    "    )\n",
    "    plt.title(\"Graph Visualization with Communities\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8IcJOiRnU-AG",
   "metadata": {
    "id": "8IcJOiRnU-AG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "class GraphRetriever:\n",
    "    def __init__(self, graph, embedding_model=\"text-embedding-3-small\"):\n",
    "        \"\"\"\n",
    "        Initialize the graph retriever.\n",
    "\n",
    "        Args:\n",
    "            graph: The graph object containing nodes and edges.\n",
    "            embedding_model: Model to use for generating embeddings.\n",
    "        \"\"\"\n",
    "        self.graph = graph\n",
    "        self.embedding_model = embedding_model\n",
    "        self.embeddings = {}\n",
    "        self.openai_client = None\n",
    "\n",
    "    def set_openai_client(self, api_key):\n",
    "        \"\"\"\n",
    "        Set up the OpenAI client.\n",
    "\n",
    "        Args:\n",
    "            api_key: OpenAI API key.\n",
    "        \"\"\"\n",
    "        self.openai_client = OpenAI(api_key=api_key)\n",
    "\n",
    "    def generate_embeddings(self):\n",
    "        \"\"\"Generate embeddings with caching.\"\"\"\n",
    "        cache_file = \"embeddings_cache.json\"\n",
    "\n",
    "        # Load cache if exists\n",
    "        if os.path.exists(cache_file):\n",
    "            with open(cache_file, 'r') as f:\n",
    "                self.embeddings = json.load(f)\n",
    "\n",
    "        # Generate missing embeddings\n",
    "        new_nodes = set(self.graph.nodes()) - set(self.embeddings.keys())\n",
    "        if new_nodes:\n",
    "            for node in new_nodes:\n",
    "                # Generate embedding as before\n",
    "                node_text = f\"{node} ({self.graph.nodes[node].get('type', 'Unknown')})\"\n",
    "                response = self.openai_client.embeddings.create(\n",
    "                    model=self.embedding_model,\n",
    "                    input=node_text,\n",
    "                    encoding_format=\"float\"\n",
    "                )\n",
    "                self.embeddings[node] = response.data[0].embedding\n",
    "\n",
    "            # Save updated cache\n",
    "            with open(cache_file, 'w') as f:\n",
    "                json.dump(self.embeddings, f)\n",
    "\n",
    "    def calculate_similarity(self, query_embedding, node_embedding):\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between query and node embeddings.\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(query_embedding, node_embedding)\n",
    "        query_norm = np.linalg.norm(query_embedding)\n",
    "        node_norm = np.linalg.norm(node_embedding)\n",
    "        return dot_product / (query_norm * node_norm)\n",
    "\n",
    "    def retrieve(self, query, k=5):\n",
    "        \"\"\"\n",
    "        Retrieve k most relevant nodes based on the query.\n",
    "\n",
    "        Args:\n",
    "            query: The search query.\n",
    "            k: Number of nodes to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            List of (node, similarity_score) tuples.\n",
    "        \"\"\"\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No embeddings generated. Call generate_embeddings first.\")\n",
    "\n",
    "        # Generate embedding for the query\n",
    "        query_response = self.openai_client.embeddings.create(\n",
    "            model=self.embedding_model,\n",
    "            input=query,\n",
    "            encoding_format=\"float\"\n",
    "        )\n",
    "        query_embedding = query_response.data[0].embedding\n",
    "\n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for node, node_embedding in self.embeddings.items():\n",
    "            similarity = self.calculate_similarity(query_embedding, node_embedding)\n",
    "            similarities.append((node, similarity))\n",
    "\n",
    "        # Sort by similarity and return top k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:k]\n",
    "\n",
    "    def retrieve_with_context(self, query, k=5, include_neighbors=True):\n",
    "        \"\"\"\n",
    "        Retrieve relevant nodes with their neighborhood context.\n",
    "\n",
    "        Args:\n",
    "            query: The search query.\n",
    "            k: Number of nodes to retrieve.\n",
    "            include_neighbors: Whether to include neighboring nodes.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with relevant nodes and their context.\n",
    "        \"\"\"\n",
    "        relevant_nodes = self.retrieve(query, k)\n",
    "\n",
    "        results = {}\n",
    "        for node, score in relevant_nodes:\n",
    "            context = {\n",
    "                'similarity_score': score,\n",
    "                'node_type': self.graph.nodes[node].get('type', 'Unknown'),\n",
    "                'neighbors': [],\n",
    "                'edges': []\n",
    "            }\n",
    "\n",
    "            if include_neighbors:\n",
    "                # Get neighboring nodes\n",
    "                neighbors = list(self.graph.neighbors(node))\n",
    "                neighbor_data = []\n",
    "                for neighbor in neighbors:\n",
    "                    neighbor_data.append({\n",
    "                        'node': neighbor,\n",
    "                        'type': self.graph.nodes[neighbor].get('type', 'Unknown')\n",
    "                    })\n",
    "                context['neighbors'] = neighbor_data\n",
    "\n",
    "                # Get edges with these neighbors\n",
    "                edges = []\n",
    "                for neighbor in neighbors:\n",
    "                    edge_data = self.graph.get_edge_data(node, neighbor)\n",
    "                    if edge_data:\n",
    "                        edges.append({\n",
    "                            'source': node,\n",
    "                            'target': neighbor,\n",
    "                            'attributes': edge_data\n",
    "                        })\n",
    "                context['edges'] = edges\n",
    "\n",
    "            results[node] = context\n",
    "\n",
    "        return results\n",
    "\n",
    "    def search(self, query, k=5, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Search the graph with a semantic query.\n",
    "\n",
    "        Args:\n",
    "            query: Search query.\n",
    "            k: Maximum number of results to return.\n",
    "            threshold: Minimum similarity score threshold.\n",
    "\n",
    "        Returns:\n",
    "            List of relevant results with context.\n",
    "        \"\"\"\n",
    "        results = self.retrieve_with_context(query, k)\n",
    "\n",
    "        # Filter by threshold\n",
    "        filtered_results = {\n",
    "            node: data\n",
    "            for node, data in results.items()\n",
    "            if data['similarity_score'] >= threshold\n",
    "        }\n",
    "\n",
    "        return filtered_results\n",
    "    def hybrid_search(self, query, k=5, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Combine semantic and structural search.\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of results\n",
    "            alpha: Weight between semantic (0) and structural (1) similarity\n",
    "        \"\"\"\n",
    "        semantic_results = self.search(query, k=k)\n",
    "\n",
    "        # Add PageRank scores for structural importance\n",
    "        pagerank_scores = nx.pagerank(self.graph)\n",
    "\n",
    "        # Combine scores\n",
    "        combined_results = {}\n",
    "        for node, data in semantic_results.items():\n",
    "            combined_score = (\n",
    "                alpha * pagerank_scores[node] +\n",
    "                (1 - alpha) * data['similarity_score']\n",
    "            )\n",
    "            data['combined_score'] = combined_score\n",
    "            combined_results[node] = data\n",
    "\n",
    "        return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ugaly1qT0e2T",
   "metadata": {
    "id": "Ugaly1qT0e2T"
   },
   "outputs": [],
   "source": [
    "# Pipeline integration code\n",
    "def run_analysis_pipeline(csv_path, openai_api_key, cache_dir=\"cache\"):\n",
    "    \"\"\"\n",
    "    Run the complete analysis pipeline with enhanced features.\n",
    "\n",
    "    Args:\n",
    "        csv_path: Path to the FAA data CSV\n",
    "        openai_api_key: OpenAI API key\n",
    "        cache_dir: Directory for caching (reserved for future use)\n",
    "    \"\"\"\n",
    "    # Create cache directory if it doesn't exist\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Data Preparation\n",
    "    print(\"Step 1: Preparing data...\")\n",
    "    data_preparer = DataPreparer(csv_path)\n",
    "    data_preparer.load_data()\n",
    "    data_preparer.clean_data()\n",
    "    data_preparer.normalize_text()\n",
    "    prepared_data = data_preparer.get_prepared_data()\n",
    "\n",
    "    # Step 2: Graph Processing\n",
    "    print(\"\\nStep 2: Building and processing graph...\")\n",
    "    graph_processor = DynamicGraphProcessor(prepared_data)\n",
    "    graph_processor.build_graph()\n",
    "\n",
    "    # Step 3: Community Detection\n",
    "    print(\"\\nStep 3: Detecting communities...\")\n",
    "    graph_processor.detect_communities()\n",
    "\n",
    "    # Step 4: Set up retriever\n",
    "    print(\"\\nStep 4: Setting up retriever and generating embeddings...\")\n",
    "    retriever = GraphRetriever(\n",
    "        graph_processor.graph,\n",
    "        embedding_model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    retriever.set_openai_client(openai_api_key)\n",
    "    retriever.generate_embeddings()\n",
    "\n",
    "    return {\n",
    "        'data_preparer': data_preparer,\n",
    "        'graph_processor': graph_processor,\n",
    "        'retriever': retriever\n",
    "    }\n",
    "\n",
    "def query_graph(retriever, query, k=10, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Query the graph and generate a coherent summary answer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Executing search...\")\n",
    "        results = retriever.search(query, k=k, threshold=threshold)\n",
    "\n",
    "        if not results:\n",
    "            return \"No relevant information found in the incident data.\"\n",
    "\n",
    "        # Collect and analyze the findings\n",
    "        findings = {\n",
    "            'incidents': [],\n",
    "            'categories': set(),\n",
    "            'contexts': set(),\n",
    "            'relationships': []\n",
    "        }\n",
    "\n",
    "        for node, data in results.items():\n",
    "            node_attrs = retriever.graph.nodes[node]\n",
    "\n",
    "            # Collect incident categories\n",
    "            if 'category' in node_attrs:\n",
    "                findings['categories'].add(node_attrs['category'])\n",
    "\n",
    "            # Collect contexts\n",
    "            if 'context' in node_attrs:\n",
    "                findings['contexts'].add(node_attrs['context'])\n",
    "\n",
    "            # Collect relationships and their contexts\n",
    "            if data.get('edges'):\n",
    "                for edge in data['edges']:\n",
    "                    attrs = edge.get('attributes', {})\n",
    "                    if 'context' in attrs:\n",
    "                        findings['relationships'].append({\n",
    "                            'source': edge['source'],\n",
    "                            'target': edge['target'],\n",
    "                            'context': attrs['context']\n",
    "                        })\n",
    "\n",
    "        # Generate a coherent summary\n",
    "        client = OpenAI(api_key=retriever.openai_client.api_key)\n",
    "\n",
    "        # Prepare the context for GPT\n",
    "        context = f\"\"\"\n",
    "            Based on the analysis of aviation incident data:\n",
    "\n",
    "            Categories of incidents: {', '.join(findings['categories'])}\n",
    "            Incident contexts: {', '.join(findings['contexts'])}\n",
    "\n",
    "            Key relationships found:\n",
    "            {chr(10).join([f\"- {r['source']} related to {r['target']}: {r['context']}\" for r in findings['relationships']])}\n",
    "\n",
    "            Original query: {query}\n",
    "            \"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an aviation safety analyst. Provide a clear, concise summary of incident data findings.\"},\n",
    "                {\"role\": \"user\", \"content\": context}\n",
    "            ],\n",
    "            max_tokens=150,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Print detailed findings for reference\n",
    "        print(\"\\nDetailed Findings:\")\n",
    "        print(\"=\"*50)\n",
    "        for node, data in results.items():\n",
    "            print(f\"\\nNode: {node}\")\n",
    "            print(f\"Similarity Score: {data['similarity_score']:.3f}\")\n",
    "            print(f\"Node Type: {data['node_type']}\")\n",
    "            if data.get('neighbors'):\n",
    "                print(\"\\nRelated factors:\")\n",
    "                for neighbor in data['neighbors']:\n",
    "                    print(f\"- {neighbor['node']}\")\n",
    "\n",
    "        print(\"\\nSummary Answer:\")\n",
    "        print(\"=\"*50)\n",
    "        print(summary)\n",
    "\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query: {e}\")\n",
    "        return f\"Error processing query: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "BgDGSDqW0mtl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BgDGSDqW0mtl",
    "outputId": "a9637cdd-1a41-4857-ca76-2d2b413379ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Preparing data...\n",
      "Data loaded successfully!\n",
      "Data cleaned successfully!\n",
      "Text fields normalized successfully!\n",
      "\n",
      "Step 2: Building and processing graph...\n",
      "Graph built successfully with 471 nodes and 450 edges!\n",
      "\n",
      "Step 3: Detecting communities...\n",
      "Detected 169 communities!\n",
      "\n",
      "Step 4: Setting up retriever and generating embeddings...\n",
      "\n",
      "Pipeline Statistics:\n",
      "------------------------------\n",
      "Total nodes in graph: 471\n",
      "Total edges in graph: 450\n",
      "Number of embeddings generated: 471\n",
      "\n",
      "Graph Analysis:\n",
      "------------------------------\n",
      "Most connected nodes:\n",
      "- pilot: 25 connections\n",
      "- engine: 20 connections\n",
      "- aircraft: 18 connections\n",
      "- fuel: 17 connections\n",
      "- door: 16 connections\n",
      "\n",
      "Executing Query: What is the most common cause of engine failure?\n",
      "--------------------------------------------------\n",
      "Executing search...\n",
      "\n",
      "Detailed Findings:\n",
      "==================================================\n",
      "\n",
      "Node: engine\n",
      "Similarity Score: 0.384\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- takeoff\n",
      "- climb\n",
      "- aircraft\n",
      "- left\n",
      "- rough\n",
      "- returned\n",
      "- climbout\n",
      "\n",
      "Node: failure\n",
      "Similarity Score: 0.317\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- power\n",
      "- engine\n",
      "\n",
      "Node: overheat\n",
      "Similarity Score: 0.311\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- engine\n",
      "\n",
      "Summary Answer:\n",
      "==================================================\n",
      "Based on the analysis of aviation incident data, the most common cause of engine failure is related to inadequate inspection of the engine. Incidents involving engine failures were often associated with issues during takeoff, climb, and climbout phases of the flight. Additionally, engine failures were linked to power failure, overheat, and rough engine performance. Emergency landings were frequently required as a result of engine-related issues.\n",
      "\n",
      "\n",
      "\n",
      "Executing Query: Describe incidents involving engine problems\n",
      "--------------------------------------------------\n",
      "Executing search...\n",
      "\n",
      "Detailed Findings:\n",
      "==================================================\n",
      "\n",
      "Node: engine\n",
      "Similarity Score: 0.430\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- takeoff\n",
      "- climb\n",
      "- aircraft\n",
      "- left\n",
      "- rough\n",
      "- returned\n",
      "- climbout\n",
      "\n",
      "Node: crashed\n",
      "Similarity Score: 0.350\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- takeoff\n",
      "- pins\n",
      "\n",
      "Node: crash\n",
      "Similarity Score: 0.328\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- initial\n",
      "- climbout\n",
      "\n",
      "Node: problems\n",
      "Similarity Score: 0.328\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- brake\n",
      "\n",
      "Node: emergency\n",
      "Similarity Score: 0.317\n",
      "Node Type: Entity\n",
      "\n",
      "Node: fuel\n",
      "Similarity Score: 0.315\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- pond\n",
      "- secured\n",
      "\n",
      "Node: overheat\n",
      "Similarity Score: 0.314\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- engine\n",
      "\n",
      "Node: failure\n",
      "Similarity Score: 0.314\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- power\n",
      "- engine\n",
      "\n",
      "Summary Answer:\n",
      "==================================================\n",
      "The analysis of aviation incident data revealed the following findings related to incidents involving engine problems:\n",
      "\n",
      "- Incidents included inadequate inspections of the aircraft and attempted operations with engine issues.\n",
      "- Incident contexts encompassed miscellaneous incidents and emergency landings.\n",
      "- Key relationships were found between engine problems and various aspects such as takeoff, climb, aircraft, left engine, rough engine during climbout, returned engine cowling, crashed during takeoff, locking pins causing crashes, initial climbout crashes, brake problems, fuel in pond, secured fuel, engine overheating, power failure leading to aerobatics and emergency landings, and engine failures during takeoff.\n",
      "\n",
      "In summary, incidents involving engine problems varied in nature, from issues during takeoff and climb to crashes and failures,\n",
      "\n",
      "\n",
      "\n",
      "Executing Query: How do weather conditions affect incidents?\n",
      "--------------------------------------------------\n",
      "Executing search...\n",
      "\n",
      "\n",
      "\n",
      "Executing Query: What are the main factors in landing incidents?\n",
      "--------------------------------------------------\n",
      "Executing search...\n",
      "\n",
      "Detailed Findings:\n",
      "==================================================\n",
      "\n",
      "Node: landing\n",
      "Similarity Score: 0.397\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- separated\n",
      "- forced\n",
      "- nose\n",
      "- airport\n",
      "\n",
      "Node: landed\n",
      "Similarity Score: 0.314\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- gear\n",
      "- umc\n",
      "- engine\n",
      "- pilot\n",
      "- power\n",
      "\n",
      "Summary Answer:\n",
      "==================================================\n",
      "Based on the analysis of aviation incident data, the main factors in landing incidents include inadequate inspection, emergency landings, issues related to separation (such as pontoon separation), forced landings, nose landings, airport landings, landing gear problems, landing on sea ice, engine issues (such as rough engine and unlocked primer), pilot-related factors (such as weather conditions and fuel concerns), and power loss leading to hard or rough landings. These key relationships highlight the various factors contributing to landing incidents in aviation safety.\n",
      "\n",
      "\n",
      "\n",
      "Executing Query: What types of pilot errors are reported?\n",
      "--------------------------------------------------\n",
      "Executing search...\n",
      "\n",
      "Detailed Findings:\n",
      "==================================================\n",
      "\n",
      "Node: pilot\n",
      "Similarity Score: 0.509\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- 4the\n",
      "- unable\n",
      "- field\n",
      "- atc\n",
      "- communication\n",
      "- altitude\n",
      "- rough\n",
      "- liftoff\n",
      "- pilot\n",
      "\n",
      "Node: pilots\n",
      "Similarity Score: 0.498\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- roll\n",
      "\n",
      "Summary Answer:\n",
      "==================================================\n",
      "Summary of Incident Data Findings:\n",
      "\n",
      "- Categories of incidents: Inadequate inspection of aircraft, attempted operation with insufficient information.\n",
      "- Incident contexts: Miscellaneous factors, weather conditions.\n",
      "- Key relationships found:\n",
      "  - Pilot errors related to various issues such as communication, altitude, rough landing, and takeoff roll.\n",
      "  - Instances of pilots being unable to pressurize and access doors.\n",
      "  - Communication issues with air traffic control (ATC) and within the pilot community.\n",
      "- Specific incidents involved pilots claiming water in fuel after rough landings and issues with liftoff and field operations.\n",
      "\n",
      "Overall, the analysis highlights the importance of proper inspection procedures, communication, and pilot training to prevent errors and enhance aviation safety.\n",
      "\n",
      "\n",
      "\n",
      "Executing Query: What safety issues are most common?\n",
      "--------------------------------------------------\n",
      "Executing search...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update the main execution part:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the pipeline\n",
    "    pipeline_results = run_analysis_pipeline(\n",
    "        csv_path=\"../../OMIn_dataset/data/FAA_data/FAA_sample_100.csv\",\n",
    "        openai_api_key=openai_api_key,\n",
    "        cache_dir=\"graph_cache\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nPipeline Statistics:\")\n",
    "    print(\"-\"*30)\n",
    "    retriever = pipeline_results['retriever']\n",
    "    graph_processor = pipeline_results['graph_processor']\n",
    "    print(f\"Total nodes in graph: {graph_processor.graph.number_of_nodes()}\")\n",
    "    print(f\"Total edges in graph: {graph_processor.graph.number_of_edges()}\")\n",
    "    print(f\"Number of embeddings generated: {len(retriever.embeddings)}\")\n",
    "\n",
    "    # Add graph analysis\n",
    "    print(\"\\nGraph Analysis:\")\n",
    "    print(\"-\"*30)\n",
    "    print(\"Most connected nodes:\")\n",
    "    degrees = sorted([(n, d) for n, d in graph_processor.graph.degree()],\n",
    "                    key=lambda x: x[1], reverse=True)[:5]\n",
    "    for node, degree in degrees:\n",
    "        print(f\"- {node}: {degree} connections\")\n",
    "\n",
    "    # Run example queries with more variation\n",
    "    queries = [\n",
    "        \"What is the most common cause of engine failure?\",\n",
    "        \"Describe incidents involving engine problems\",\n",
    "        \"How do weather conditions affect incidents?\",\n",
    "        \"What are the main factors in landing incidents?\",\n",
    "        \"What types of pilot errors are reported?\",\n",
    "        \"What safety issues are most common?\"\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\nExecuting Query: {query}\")\n",
    "        print(\"-\"*50)\n",
    "        results = query_graph(\n",
    "            pipeline_results['retriever'],\n",
    "            query,\n",
    "            k=8,  # Increased number of results\n",
    "            threshold=0.3  # Lowered threshold\n",
    "        )\n",
    "\n",
    "        if not results:\n",
    "            print(\"No results found for this query.\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bqc3PRpG8ZUR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqc3PRpG8ZUR",
    "outputId": "25cd781a-4149-42cb-9bde-d735e6bcc496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing search...\n",
      "\n",
      "Detailed Findings:\n",
      "==================================================\n",
      "\n",
      "Node: engine\n",
      "Similarity Score: 0.384\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- takeoff\n",
      "- climb\n",
      "- aircraft\n",
      "- left\n",
      "- rough\n",
      "- returned\n",
      "- climbout\n",
      "\n",
      "Node: failure\n",
      "Similarity Score: 0.317\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- power\n",
      "- engine\n",
      "\n",
      "Node: overheat\n",
      "Similarity Score: 0.311\n",
      "Node Type: Entity\n",
      "\n",
      "Related factors:\n",
      "- engine\n",
      "\n",
      "Summary Answer:\n",
      "==================================================\n",
      "Based on the analysis of aviation incident data, the most common cause of engine failure is related to inadequate inspection. Incidents involving engine failure during takeoff, climb, and in-flight were frequently reported, with issues such as rough engine performance, engine power failure, engine overheat, and engine separation being significant factors. Additionally, emergency landings were often necessitated by engine-related problems, highlighting the critical importance of proper engine maintenance and inspection procedures in aviation safety.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"What is the most common cause of engine failure?\"\n",
    "summary = query_graph(retriever, query)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "keo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
