{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0815c380-df2f-475a-925c-2da2ee3d1c45",
   "metadata": {},
   "source": [
    "## NER GS Overlap\n",
    "\n",
    "This notebook finds the overlap of entities in the benchmark-annotated NER GSs with our un-typed NER GS\n",
    "\n",
    "We summarize:\n",
    "- the total number of entities generated by each benchmark-annotated GS\n",
    "- the number of entities in each benchmark-annotated GS that match or partially match an entity in our GS, regardless of label\n",
    "- the sum of the matches and partial matches divided by the total number of entities in our un-typed GS\n",
    "\n",
    "To do this, we use the NER eval kit developed by davidsbatista at https://github.com/davidsbatista/NER-Evaluation/tree/master, used in the NER evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d7389a9-c1ca-4229-81f8-9adbad70706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../evaluations/automatic/NER-Evaluation\")\n",
    "sys.path.append(\"../evaluations/automatic\")\n",
    "from ner_semeval import get_faa_tokenized, get_true_pred_ents, check_named_entities, eval\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12e5e670-3021-483c-bdb5-414ec830f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_tags = ['PER', 'ORG', 'MISC', 'LOC']\n",
    "ace_nltk_tags = ['PER','ORG','LOC','FAC','GPE'] # RESTRICTED SET\n",
    "ace_tags = ['PER','ORG','LOC','FAC','GPE','VEHICLE','WEAPON']\n",
    "on_tags = ['PER','ORG','LOC','FAC','GPE','PRODUCT','NORP','QUANTITY','EVENT','WORK_OF_ART','CARDINAL','DATE','PERCENT','TIME','ORDINAL','MONEY','LAW','LANGUAGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c26b0b8-7a6e-4526-993b-af23211f83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_data = {'Conll-2003':{'tags':conll_tags, 'path':'processed/ner_conll.csv'},'ACE-2005':{'tags':ace_tags,'path':'processed/ner_ace.csv'},'ACE Phase 1':{'tags':ace_nltk_tags, 'path':'processed/ner_ace_nltk.csv'},'OntoNotes 5.0':{'tags':on_tags,'path':'processed/ner_on.csv'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dab28d02-b0da-4dd3-b3f3-c0f64be1d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for benchmark in bench_data:\n",
    "    # Mostly copied from ner_semeval.main: #####\n",
    "    \n",
    "    faa = get_faa_tokenized('../data/FAA_data/faa.conll')\n",
    "    \n",
    "    utfaa = pd.read_csv('processed/ner.csv')\n",
    "    utfaa['labels'] = ['ORG']*len(utfaa) # dummy labels to not break the script\n",
    "    bench = pd.read_csv(bench_data[benchmark]['path'])\n",
    "    \n",
    "    all_true_ents, all_pred_ents = get_true_pred_ents(utfaa, bench, faa)\n",
    "    \n",
    "    # Check that true and pred ents were processed without error\n",
    "    for named_entities, df in zip([all_true_ents, all_pred_ents],[utfaa, bench]):\n",
    "        probs = check_named_entities(named_entities, utfaa['id'].unique(), df)\n",
    "        if len(probs) > 0:\n",
    "            print(f\"Warning: The following mentions could not be matched to span indices in documents. Ignore if none of these are present in GS: {probs}\")\n",
    "    \n",
    "    tags = bench_data[benchmark]['tags']\n",
    "    \n",
    "    results = eval(all_true_ents, all_pred_ents, tags)\n",
    "    \n",
    "    ###################\n",
    "    \n",
    "    # Use results['partial'] to get stats:\n",
    "    bench_data[benchmark]['total'] = f\"{results['partial']['actual']}\"\n",
    "    bench_data[benchmark]['match'] = f\"{results['partial']['correct']}\"\n",
    "    bench_data[benchmark]['partial'] = f\"{results['partial']['partial']}\"\n",
    "    bench_data[benchmark]['overlap'] = f\"{(match + partial)/results['partial']['possible']:.2}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0705da81-0dec-4433-8ab0-9a02c527e807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|               | Total | Match | Partial | Overlap |\n",
      "|---------------|-------|-------|---------|---------|\n",
      "| Conll-2003    | 44    | 36    | 8       | 0.12    |\n",
      "| ACE-2005      | 195   | 133   | 54      | 0.11    |\n",
      "| ACE Phase 1   | 122   | 89    | 26      | 0.12    |\n",
      "| OntoNotes 5.0 | 61    | 52    | 9       | 0.12    |\n"
     ]
    }
   ],
   "source": [
    "# Print out:\n",
    "\n",
    "print(\"|               | Total | Match | Partial | Overlap |\")\n",
    "print(\"|---------------|-------|-------|---------|---------|\")\n",
    "for benchmark in bench_data:\n",
    "    print(f\"| {benchmark:13} | {bench_data[benchmark]['total']:6}| {bench_data[benchmark]['match']:6}| {bench_data[benchmark]['partial']:8}| {bench_data[benchmark]['overlap']:8}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fbcffd-b571-40c7-b426-4b684c807bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
