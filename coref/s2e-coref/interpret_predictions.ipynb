{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71828882-311d-4e39-ab3e-bad266d47ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "11fa629a-37ef-448e-a389-7a1edf7e24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimized = []\n",
    "with open(f\"data/test.english.jsonlines\") as f:\n",
    "        data_lines = f.readlines()\n",
    "        for line in data_lines:\n",
    "            minimized.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6be6dfb3-3699-4f7b-8a7f-dc32d149d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "with open(f\"output/preds.jsonl\") as f:\n",
    "        data_lines = f.readlines()\n",
    "        for line in data_lines:\n",
    "            preds.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "89ce0065-bc87-4a6c-97e2-7ccb9ce53bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_prediction, doc_to_subtoken_map = preds\n",
    "keys = sorted(list(doc_to_prediction.keys()), key = lambda x: int(x.split('_')[0].split('/')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cb7ad238-0aa3-4d38-98b8-83270b4c7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "subtoken_map = {}\n",
    "for key in keys:\n",
    "    idx_key = f\"faa_{key.split('_')[0].split('/')[1]}_0\"\n",
    "    predictions[idx_key] = doc_to_prediction[key]\n",
    "    subtoken_map[idx_key] = doc_to_subtoken_map[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "855c5d99-ea4b-4920-b2e3-8a1413896181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using code taken from s2e-coref/conll.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae82a04-1530-4daa-ba42-575449883c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections, operator, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa78fcd1-c9b7-419c-ac50-22038dd01bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGIN_DOCUMENT_REGEX = re.compile(r\"#begin document \\((.*)\\); part (\\d+)\")  # First line at each document\n",
    "COREF_RESULTS_REGEX = re.compile(r\".*Coreference: Recall: \\([0-9.]+ / [0-9.]+\\) ([0-9.]+)%\\tPrecision: \\([0-9.]+ / [0-9.]+\\) ([0-9.]+)%\\tF1: ([0-9.]+)%.*\", re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05949446-c87d-4759-b011-47aefc8cddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_key(doc_id, part):\n",
    "    return \"{}_{}\".format(doc_id, int(part))\n",
    "\n",
    "def output_conll(input_file, output_file, predictions, subtoken_map):\n",
    "    prediction_map = {}\n",
    "    for doc_key, clusters in predictions.items():\n",
    "        start_map = collections.defaultdict(list)\n",
    "        end_map = collections.defaultdict(list)\n",
    "        word_map = collections.defaultdict(list)\n",
    "        for cluster_id, mentions in enumerate(clusters):\n",
    "            for start, end in mentions:\n",
    "                start, end = subtoken_map[doc_key][start], subtoken_map[doc_key][end]\n",
    "                if start == end:\n",
    "                    word_map[start].append(cluster_id)\n",
    "                else:\n",
    "                    start_map[start].append((cluster_id, end))\n",
    "                    end_map[end].append((cluster_id, start))\n",
    "        for k,v in start_map.items():\n",
    "            start_map[k] = [cluster_id for cluster_id, end in sorted(v, key=operator.itemgetter(1), reverse=True)]\n",
    "        for k,v in end_map.items():\n",
    "            end_map[k] = [cluster_id for cluster_id, start in sorted(v, key=operator.itemgetter(1), reverse=True)]\n",
    "        prediction_map[doc_key] = (start_map, end_map, word_map)\n",
    "\n",
    "    word_index = 0\n",
    "    for line in input_file.readlines():\n",
    "        row = line.split()\n",
    "        if len(row) == 0:\n",
    "            output_file.write(\"\\n\")\n",
    "        elif row[0].startswith(\"#\"):\n",
    "            begin_match = re.match(BEGIN_DOCUMENT_REGEX, line)\n",
    "            if begin_match:\n",
    "                doc_key = get_doc_key(begin_match.group(1), begin_match.group(2))\n",
    "                start_map, end_map, word_map = prediction_map[doc_key]\n",
    "                word_index = 0\n",
    "            output_file.write(line)\n",
    "            output_file.write(\"\\n\")\n",
    "        else:\n",
    "            assert get_doc_key(row[0], row[1]) == doc_key\n",
    "            coref_list = []\n",
    "            if word_index in end_map:\n",
    "                for cluster_id in end_map[word_index]:\n",
    "                    coref_list.append(\"{})\".format(cluster_id))\n",
    "            if word_index in word_map:\n",
    "                for cluster_id in word_map[word_index]:\n",
    "                    coref_list.append(\"({})\".format(cluster_id))\n",
    "            if word_index in start_map:\n",
    "                for cluster_id in start_map[word_index]:\n",
    "                    coref_list.append(\"({}\".format(cluster_id))\n",
    "\n",
    "            if len(coref_list) == 0:\n",
    "                row[-1] = \"-\"\n",
    "            else:\n",
    "                row[-1] = \"|\".join(coref_list)\n",
    "\n",
    "            output_file.write(\"   \".join(row))\n",
    "            output_file.write(\"\\n\")\n",
    "            word_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10fd1ca3-bdf5-4646-95bb-3c239dc23ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_file = open('data/test.english.v4_gold_conll')\n",
    "output_file = open('output/preds.conll','w')\n",
    "output_conll(input_file, output_file, predictions, subtoken_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2056f46c-9a1e-4cfd-bd01-6557a07bedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_map = {}\n",
    "for doc_key, clusters in predictions.items():\n",
    "    start_map = collections.defaultdict(list)\n",
    "    end_map = collections.defaultdict(list)\n",
    "    word_map = collections.defaultdict(list)\n",
    "    for cluster_id, mentions in enumerate(clusters):\n",
    "        for start, end in mentions:\n",
    "            start, end = subtoken_map[doc_key][start], subtoken_map[doc_key][end]\n",
    "            if start == end:\n",
    "                word_map[start].append(cluster_id)\n",
    "            else:\n",
    "                start_map[start].append((cluster_id, end))\n",
    "                end_map[end].append((cluster_id, start))\n",
    "    for k,v in start_map.items():\n",
    "        start_map[k] = [cluster_id for cluster_id, end in sorted(v, key=operator.itemgetter(1), reverse=True)]\n",
    "    for k,v in end_map.items():\n",
    "        end_map[k] = [cluster_id for cluster_id, start in sorted(v, key=operator.itemgetter(1), reverse=True)]\n",
    "    prediction_map[doc_key] = (start_map, end_map, word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60c23a98-614a-470d-98e8-19b913b38453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "faa_df = pd.read_csv('../../data/FAA_data/Maintenance_Text_data_nona.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "70edfe2d-4f52-4796-856c-93eee750871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdict = {'id':[], 'sample':[], 'start_map':[], 'end_map':[], 'word_map':[], 'corefs':[]}\n",
    "\n",
    "for idoc in range(len(faa_df)):\n",
    "    if keys[idoc].split('/')[1].split('_')[1] != faa_df['c5'].iat[idoc]:\n",
    "        print(\"we have a problem\")\n",
    "    \n",
    "    outdict['id'].append(faa_df['c5'].iat[idoc])\n",
    "    outdict['sample'].append(faa_df['c119'].iat[idoc])\n",
    "    outdict['start_map'].append(dict(prediction_map[f'faa_{idoc}_0'][0]))\n",
    "    outdict['end_map'].append(dict(prediction_map[f'faa_{idoc}_0'][1]))\n",
    "    outdict['word_map'].append(dict(prediction_map[f'faa_{idoc}_0'][2]))\n",
    "    \n",
    "    part = []\n",
    "    for sentence in minimized[idoc]['sentences']:\n",
    "        part = part + sentence\n",
    "\n",
    "    # save to new dicts for easier access\n",
    "    starts = {}\n",
    "    for start, word_idx_list in prediction_map[f'faa_{idoc}_0'][0].items():\n",
    "        for word_idx in word_idx_list:\n",
    "            starts[word_idx] = starts.get(word_idx, []) + [start]\n",
    "    \n",
    "    ends = {}\n",
    "    for end, word_idx_list in prediction_map[f'faa_{idoc}_0'][1].items():\n",
    "        for word_idx in word_idx_list:\n",
    "            ends[word_idx] = ends.get(word_idx, []) + [end]\n",
    "    \n",
    "    for word, word_idx_list in prediction_map[f'faa_{idoc}_0'][2].items():\n",
    "        for word_idx in word_idx_list:\n",
    "            starts[word_idx] = starts.get(word_idx, []) + [word]\n",
    "            ends[word_idx] = ends.get(word_idx, []) + [word]\n",
    "\n",
    "    \n",
    "    corefs = {}\n",
    "    for word_idx in starts.keys():\n",
    "        for ispan in range(len(starts[word_idx])):\n",
    "            start = starts[word_idx][ispan]\n",
    "            end = ends[word_idx][ispan]\n",
    "            corefs[word_idx] = corefs.get(word_idx, []) + [' '.join(part[start:end+1])]\n",
    "    \n",
    "    outdict['corefs'].append(corefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8afe947b-bc1a-4470-a60c-355fc84bfe45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(outdict).to_csv('../../data/results/s2e-coref/s2e-coref.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d610d-36db-4c43-9108-48ed18f61824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
