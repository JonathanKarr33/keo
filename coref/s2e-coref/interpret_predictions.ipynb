{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71828882-311d-4e39-ab3e-bad266d47ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11fa629a-37ef-448e-a389-7a1edf7e24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimized = []\n",
    "with open(f\"data/test.english.jsonlines\") as f:\n",
    "        data_lines = f.readlines()\n",
    "        for line in data_lines:\n",
    "            minimized.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6be6dfb3-3699-4f7b-8a7f-dc32d149d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "with open(f\"output/preds.jsonl\") as f:\n",
    "        data_lines = f.readlines()\n",
    "        for line in data_lines:\n",
    "            preds.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89ce0065-bc87-4a6c-97e2-7ccb9ce53bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_prediction, doc_to_subtoken_map = preds\n",
    "keys = sorted(list(doc_to_prediction.keys()), key = lambda x: int(x.split('_')[0].split('/')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb7ad238-0aa3-4d38-98b8-83270b4c7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "subtoken_map = {}\n",
    "for key in keys:\n",
    "    idx_key = f\"faa_{key.split('_')[0].split('/')[1]}_0\"\n",
    "    predictions[idx_key] = doc_to_prediction[key]\n",
    "    subtoken_map[idx_key] = doc_to_subtoken_map[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bab5e404-0f0d-4470-a6ad-96c11648e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = doc_to_prediction\n",
    "subtoken_map = doc_to_subtoken_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "855c5d99-ea4b-4920-b2e3-8a1413896181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using code taken from s2e-coref/conll.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ae82a04-1530-4daa-ba42-575449883c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections, operator, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa78fcd1-c9b7-419c-ac50-22038dd01bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGIN_DOCUMENT_REGEX = re.compile(r\"#begin document \\((.*)\\); part (\\d+)\")  # First line at each document\n",
    "COREF_RESULTS_REGEX = re.compile(r\".*Coreference: Recall: \\([0-9.]+ / [0-9.]+\\) ([0-9.]+)%\\tPrecision: \\([0-9.]+ / [0-9.]+\\) ([0-9.]+)%\\tF1: ([0-9.]+)%.*\", re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "313867d6-e30b-4a8b-a4be-72419b1705b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_map = {}\n",
    "for doc_key, clusters in predictions.items():\n",
    "    start_map = collections.defaultdict(list)\n",
    "    end_map = collections.defaultdict(list)\n",
    "    word_map = collections.defaultdict(list)\n",
    "    for cluster_id, mentions in enumerate(clusters):\n",
    "        for start, end in mentions:\n",
    "            start, end = subtoken_map[doc_key][start], subtoken_map[doc_key][end]\n",
    "            if start == end:\n",
    "                word_map[start].append(cluster_id)\n",
    "            else:\n",
    "                start_map[start].append((cluster_id, end))\n",
    "                end_map[end].append((cluster_id, start))\n",
    "    for k,v in start_map.items():\n",
    "        start_map[k] = [cluster_id for cluster_id, end in sorted(v, key=operator.itemgetter(1), reverse=True)]\n",
    "    for k,v in end_map.items():\n",
    "        end_map[k] = [cluster_id for cluster_id, start in sorted(v, key=operator.itemgetter(1), reverse=True)]\n",
    "    prediction_map[doc_key] = (start_map, end_map, word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05949446-c87d-4759-b011-47aefc8cddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_key(doc_id, part):\n",
    "    return \"{}_{}\".format(doc_id, int(part))\n",
    "\n",
    "def output_conll(input_file, output_file, prediction_map):\n",
    "\n",
    "    word_index = 0\n",
    "    for line in input_file.readlines():\n",
    "        row = line.split()\n",
    "        if len(row) == 0:\n",
    "            output_file.write(\"\\n\")\n",
    "        elif row[0].startswith(\"#\"):\n",
    "            begin_match = re.match(BEGIN_DOCUMENT_REGEX, line)\n",
    "            if begin_match:\n",
    "                doc_key = get_doc_key(begin_match.group(1), begin_match.group(2))\n",
    "                start_map, end_map, word_map = prediction_map[doc_key]\n",
    "                word_index = 0\n",
    "            output_file.write(line)\n",
    "            output_file.write(\"\\n\")\n",
    "        else:\n",
    "            assert get_doc_key(row[0], row[1]) == doc_key\n",
    "            coref_list = []\n",
    "            if word_index in end_map:\n",
    "                for cluster_id in end_map[word_index]:\n",
    "                    coref_list.append(\"{})\".format(cluster_id))\n",
    "            if word_index in word_map:\n",
    "                for cluster_id in word_map[word_index]:\n",
    "                    coref_list.append(\"({})\".format(cluster_id))\n",
    "            if word_index in start_map:\n",
    "                for cluster_id in start_map[word_index]:\n",
    "                    coref_list.append(\"({}\".format(cluster_id))\n",
    "\n",
    "            if len(coref_list) == 0:\n",
    "                row[-1] = \"-\"\n",
    "            else:\n",
    "                row[-1] = \"|\".join(coref_list)\n",
    "\n",
    "            output_file.write(\"   \".join(row))\n",
    "            output_file.write(\"\\n\")\n",
    "            word_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10fd1ca3-bdf5-4646-95bb-3c239dc23ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_file = open('data/test.english.v4_gold_conll')\n",
    "output_file = open('output/preds.conll','w')\n",
    "output_conll(input_file, output_file, prediction_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60c23a98-614a-470d-98e8-19b913b38453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "faa_df = pd.read_csv('../../data/FAA_data/Maintenance_Text_data_nona.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70edfe2d-4f52-4796-856c-93eee750871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdict = {'c5_id':[], 'c119_input':[], 'start_map':[], 'end_map':[], 'word_map':[], 'corefs_human_readable':[],'corefs':[]}\n",
    "\n",
    "for idoc in range(len(faa_df)):\n",
    "\n",
    "    key = keys[idoc]\n",
    "    \n",
    "    if key.split('/')[1].split('_')[1] != faa_df['c5'].iat[idoc]:\n",
    "        print(\"we have a problem\")\n",
    "    \n",
    "    outdict['c5_id'].append(faa_df['c5'].iat[idoc])\n",
    "    outdict['c119_input'].append(faa_df['c119'].iat[idoc])\n",
    "    outdict['start_map'].append(dict(prediction_map[key][0]))\n",
    "    outdict['end_map'].append(dict(prediction_map[key][1]))\n",
    "    outdict['word_map'].append(dict(prediction_map[key][2]))\n",
    "    \n",
    "    part = []\n",
    "    for sentence in minimized[idoc]['sentences']:\n",
    "        part = part + sentence\n",
    "\n",
    "    # save to new dicts for easier access\n",
    "    starts = {}\n",
    "    for start, word_idx_list in prediction_map[key][0].items():\n",
    "        for word_idx in word_idx_list:\n",
    "            starts[word_idx] = starts.get(word_idx, []) + [start]\n",
    "    \n",
    "    ends = {}\n",
    "    for end, word_idx_list in prediction_map[key][1].items():\n",
    "        for word_idx in word_idx_list:\n",
    "            ends[word_idx] = ends.get(word_idx, []) + [end]\n",
    "    \n",
    "    for word, word_idx_list in prediction_map[key][2].items():\n",
    "        for word_idx in word_idx_list:\n",
    "            starts[word_idx] = starts.get(word_idx, []) + [word]\n",
    "            ends[word_idx] = ends.get(word_idx, []) + [word]\n",
    "\n",
    "    \n",
    "    human_corefs = {}\n",
    "    corefs = {}\n",
    "    for word_idx in starts.keys():\n",
    "        for ispan in range(len(starts[word_idx])):\n",
    "            start = starts[word_idx][ispan]\n",
    "            end = ends[word_idx][ispan]\n",
    "            human_corefs[word_idx] = human_corefs.get(word_idx, []) + [' '.join(part[start:end+1])]\n",
    "            corefs[word_idx] = corefs.get(word_idx, []) + [[start, end]]\n",
    "    \n",
    "    outdict['corefs_human_readable'].append(human_corefs)\n",
    "    outdict['corefs'].append(list(corefs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8afe947b-bc1a-4470-a60c-355fc84bfe45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(outdict).to_csv('../../data/results/s2e-coref/s2e-coref_updated_format.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c74d610d-36db-4c43-9108-48ed18f61824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c5_id</th>\n",
       "      <th>c119_input</th>\n",
       "      <th>start_map</th>\n",
       "      <th>end_map</th>\n",
       "      <th>word_map</th>\n",
       "      <th>corefs_human_readable</th>\n",
       "      <th>corefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>19950826026019A</td>\n",
       "      <td>EXPLOSION LIFTING LOGS. PITCHED UP, ROLLED. CO...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                c5_id                                         c119_input  \\\n",
       "1920  19950826026019A  EXPLOSION LIFTING LOGS. PITCHED UP, ROLLED. CO...   \n",
       "\n",
       "     start_map end_map word_map corefs_human_readable corefs  \n",
       "1920        {}      {}       {}                    {}     []  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df = pd.DataFrame(outdict)\n",
    "out_df[out_df['c5_id'] == '19950826026019A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14fb370-033e-41d3-989e-3a83038cf0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
