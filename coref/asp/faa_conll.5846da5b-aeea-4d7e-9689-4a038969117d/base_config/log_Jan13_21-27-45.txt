Log file path: ./data/faa_conll/base_config/log_Jan13_21-27-45.txt
Config:
task: coref
dataset: faa_conll
data_dir: ./data/faa_conll/
model_dir: ./data/faa_conll/
log_root: ./data/faa_conll/
max_segment_len: 2048
use_amp: True
optimizer: adamw
plm_learning_rate: 3e-05
task_learning_rate: 0.0003
plm_scheduler: linear_with_warmup
task_scheduler: linear_with_warmup
warmup_ratio: 0.1
adam_eps: 1e-08
adam_weight_decay: 0.1
max_grad_norm: 1
batch_size: 1
gradient_accumulation_steps: 1
num_epochs: 40
activation: relu
init_std: 0.02
dropout_rate: 0.3
feature_emb_size: 20
hidden_size: 2048
beam_size: 1
plm_pretrained_name_or_path: t5-base
plm_tokenizer_name: t5-small
eval_frequency: 2000
report_frequency: 100
log_dir: ./data/faa_conll/base_config
tb_dir: ./data/faa_conll/tensorboard
Model parameters:
model.t5.shared.weight: (32104, 768)
model.t5.encoder.block.0.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.encoder.block.0.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.encoder.block.0.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.encoder.block.0.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: (32, 12)
model.t5.encoder.block.0.layer.0.layer_norm.weight: (768,)
model.t5.encoder.block.0.layer.1.DenseReluDense.wi.weight: (3072, 768)
model.t5.encoder.block.0.layer.1.DenseReluDense.wo.weight: (768, 3072)
model.t5.encoder.block.0.layer.1.layer_norm.weight: (768,)
model.t5.encoder.block.1.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.encoder.block.1.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.encoder.block.1.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.encoder.block.1.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.encoder.block.1.layer.0.layer_norm.weight: (768,)
model.t5.encoder.block.1.layer.1.DenseReluDense.wi.weight: (3072, 768)
model.t5.encoder.block.1.layer.1.DenseReluDense.wo.weight: (768, 3072)
model.t5.encoder.block.1.layer.1.layer_norm.weight: (768,)
model.t5.encoder.block.2.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.encoder.block.2.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.encoder.block.2.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.encoder.block.2.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.encoder.block.2.layer.0.layer_norm.weight: (768,)
model.t5.encoder.block.2.layer.1.DenseReluDense.wi.weight: (3072, 768)
model.t5.encoder.block.2.layer.1.DenseReluDense.wo.weight: (768, 3072)
model.t5.encoder.block.2.layer.1.layer_norm.weight: (768,)
model.t5.encoder.block.3.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.encoder.block.3.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.encoder.block.3.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.encoder.block.3.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.encoder.block.3.layer.0.layer_norm.weight: (768,)
model.t5.encoder.block.3.layer.1.DenseReluDense.wi.weight: (3072, 768)
model.t5.encoder.block.3.layer.1.DenseReluDense.wo.weight: (768, 3072)
model.t5.encoder.block.3.layer.1.layer_norm.weight: (768,)
model.t5.encoder.block.4.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.encoder.block.4.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.encoder.block.4.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.encoder.block.4.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.encoder.block.4.layer.0.layer_norm.weight: (768,)
model.t5.encoder.block.4.layer.1.DenseReluDense.wi.weight: (3072, 768)
model.t5.encoder.block.4.layer.1.DenseReluDense.wo.weight: (768, 3072)
model.t5.encoder.block.4.layer.1.layer_norm.weight: (768,)
model.t5.encoder.block.5.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.encoder.block.5.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.encoder.block.5.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.encoder.block.5.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.encoder.block.5.layer.0.layer_norm.weight: (768,)
model.t5.encoder.block.5.layer.1.DenseReluDense.wi.weight: (3072, 768)
model.t5.encoder.block.5.layer.1.DenseReluDense.wo.weight: (768, 3072)
model.t5.encoder.block.5.layer.1.layer_norm.weight: (768,)
model.t5.encoder.block.6.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.encoder.block.6.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.encoder.block.6.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.encoder.block.6.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.encoder.block.6.layer.0.layer_norm.weight: (768,)
model.t5.encoder.block.6.layer.1.DenseReluDense.wi.weight: (3072, 768)
model.t5.encoder.block.6.layer.1.DenseReluDense.wo.weight: (768, 3072)
model.t5.encoder.block.6.layer.1.layer_norm.weight: (768,)
model.t5.encoder.block.7.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.encoder.block.7.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.encoder.block.7.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.encoder.block.7.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.encoder.block.7.layer.0.layer_norm.weight: (768,)
model.t5.encoder.block.7.layer.1.DenseReluDense.wi.weight: (3072, 768)
model.t5.encoder.block.7.layer.1.DenseReluDense.wo.weight: (768, 3072)
model.t5.encoder.block.7.layer.1.layer_norm.weight: (768,)
model.t5.encoder.block.8.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.encoder.block.8.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.encoder.block.8.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.encoder.block.8.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.encoder.block.8.layer.0.layer_norm.weight: (768,)
model.t5.encoder.block.8.layer.1.DenseReluDense.wi.weight: (3072, 768)
model.t5.encoder.block.8.layer.1.DenseReluDense.wo.weight: (768, 3072)
model.t5.encoder.block.8.layer.1.layer_norm.weight: (768,)
model.t5.encoder.block.9.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.encoder.block.9.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.encoder.block.9.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.encoder.block.9.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.encoder.block.9.layer.0.layer_norm.weight: (768,)
model.t5.encoder.block.9.layer.1.DenseReluDense.wi.weight: (3072, 768)
model.t5.encoder.block.9.layer.1.DenseReluDense.wo.weight: (768, 3072)
model.t5.encoder.block.9.layer.1.layer_norm.weight: (768,)
model.t5.encoder.block.10.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.encoder.block.10.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.encoder.block.10.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.encoder.block.10.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.encoder.block.10.layer.0.layer_norm.weight: (768,)
model.t5.encoder.block.10.layer.1.DenseReluDense.wi.weight: (3072, 768)
model.t5.encoder.block.10.layer.1.DenseReluDense.wo.weight: (768, 3072)
model.t5.encoder.block.10.layer.1.layer_norm.weight: (768,)
model.t5.encoder.block.11.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.encoder.block.11.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.encoder.block.11.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.encoder.block.11.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.encoder.block.11.layer.0.layer_norm.weight: (768,)
model.t5.encoder.block.11.layer.1.DenseReluDense.wi.weight: (3072, 768)
model.t5.encoder.block.11.layer.1.DenseReluDense.wo.weight: (768, 3072)
model.t5.encoder.block.11.layer.1.layer_norm.weight: (768,)
model.t5.encoder.final_layer_norm.weight: (768,)
model.t5.decoder.block.0.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.decoder.block.0.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.decoder.block.0.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.decoder.block.0.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: (32, 12)
model.t5.decoder.block.0.layer.0.layer_norm.weight: (768,)
model.t5.decoder.block.0.layer.1.EncDecAttention.q.weight: (768, 768)
model.t5.decoder.block.0.layer.1.EncDecAttention.k.weight: (768, 768)
model.t5.decoder.block.0.layer.1.EncDecAttention.v.weight: (768, 768)
model.t5.decoder.block.0.layer.1.EncDecAttention.o.weight: (768, 768)
model.t5.decoder.block.0.layer.1.layer_norm.weight: (768,)
model.t5.decoder.block.0.layer.2.DenseReluDense.wi.weight: (3072, 768)
model.t5.decoder.block.0.layer.2.DenseReluDense.wo.weight: (768, 3072)
model.t5.decoder.block.0.layer.2.layer_norm.weight: (768,)
model.t5.decoder.block.1.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.decoder.block.1.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.decoder.block.1.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.decoder.block.1.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.decoder.block.1.layer.0.layer_norm.weight: (768,)
model.t5.decoder.block.1.layer.1.EncDecAttention.q.weight: (768, 768)
model.t5.decoder.block.1.layer.1.EncDecAttention.k.weight: (768, 768)
model.t5.decoder.block.1.layer.1.EncDecAttention.v.weight: (768, 768)
model.t5.decoder.block.1.layer.1.EncDecAttention.o.weight: (768, 768)
model.t5.decoder.block.1.layer.1.layer_norm.weight: (768,)
model.t5.decoder.block.1.layer.2.DenseReluDense.wi.weight: (3072, 768)
model.t5.decoder.block.1.layer.2.DenseReluDense.wo.weight: (768, 3072)
model.t5.decoder.block.1.layer.2.layer_norm.weight: (768,)
model.t5.decoder.block.2.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.decoder.block.2.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.decoder.block.2.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.decoder.block.2.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.decoder.block.2.layer.0.layer_norm.weight: (768,)
model.t5.decoder.block.2.layer.1.EncDecAttention.q.weight: (768, 768)
model.t5.decoder.block.2.layer.1.EncDecAttention.k.weight: (768, 768)
model.t5.decoder.block.2.layer.1.EncDecAttention.v.weight: (768, 768)
model.t5.decoder.block.2.layer.1.EncDecAttention.o.weight: (768, 768)
model.t5.decoder.block.2.layer.1.layer_norm.weight: (768,)
model.t5.decoder.block.2.layer.2.DenseReluDense.wi.weight: (3072, 768)
model.t5.decoder.block.2.layer.2.DenseReluDense.wo.weight: (768, 3072)
model.t5.decoder.block.2.layer.2.layer_norm.weight: (768,)
model.t5.decoder.block.3.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.decoder.block.3.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.decoder.block.3.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.decoder.block.3.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.decoder.block.3.layer.0.layer_norm.weight: (768,)
model.t5.decoder.block.3.layer.1.EncDecAttention.q.weight: (768, 768)
model.t5.decoder.block.3.layer.1.EncDecAttention.k.weight: (768, 768)
model.t5.decoder.block.3.layer.1.EncDecAttention.v.weight: (768, 768)
model.t5.decoder.block.3.layer.1.EncDecAttention.o.weight: (768, 768)
model.t5.decoder.block.3.layer.1.layer_norm.weight: (768,)
model.t5.decoder.block.3.layer.2.DenseReluDense.wi.weight: (3072, 768)
model.t5.decoder.block.3.layer.2.DenseReluDense.wo.weight: (768, 3072)
model.t5.decoder.block.3.layer.2.layer_norm.weight: (768,)
model.t5.decoder.block.4.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.decoder.block.4.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.decoder.block.4.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.decoder.block.4.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.decoder.block.4.layer.0.layer_norm.weight: (768,)
model.t5.decoder.block.4.layer.1.EncDecAttention.q.weight: (768, 768)
model.t5.decoder.block.4.layer.1.EncDecAttention.k.weight: (768, 768)
model.t5.decoder.block.4.layer.1.EncDecAttention.v.weight: (768, 768)
model.t5.decoder.block.4.layer.1.EncDecAttention.o.weight: (768, 768)
model.t5.decoder.block.4.layer.1.layer_norm.weight: (768,)
model.t5.decoder.block.4.layer.2.DenseReluDense.wi.weight: (3072, 768)
model.t5.decoder.block.4.layer.2.DenseReluDense.wo.weight: (768, 3072)
model.t5.decoder.block.4.layer.2.layer_norm.weight: (768,)
model.t5.decoder.block.5.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.decoder.block.5.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.decoder.block.5.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.decoder.block.5.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.decoder.block.5.layer.0.layer_norm.weight: (768,)
model.t5.decoder.block.5.layer.1.EncDecAttention.q.weight: (768, 768)
model.t5.decoder.block.5.layer.1.EncDecAttention.k.weight: (768, 768)
model.t5.decoder.block.5.layer.1.EncDecAttention.v.weight: (768, 768)
model.t5.decoder.block.5.layer.1.EncDecAttention.o.weight: (768, 768)
model.t5.decoder.block.5.layer.1.layer_norm.weight: (768,)
model.t5.decoder.block.5.layer.2.DenseReluDense.wi.weight: (3072, 768)
model.t5.decoder.block.5.layer.2.DenseReluDense.wo.weight: (768, 3072)
model.t5.decoder.block.5.layer.2.layer_norm.weight: (768,)
model.t5.decoder.block.6.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.decoder.block.6.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.decoder.block.6.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.decoder.block.6.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.decoder.block.6.layer.0.layer_norm.weight: (768,)
model.t5.decoder.block.6.layer.1.EncDecAttention.q.weight: (768, 768)
model.t5.decoder.block.6.layer.1.EncDecAttention.k.weight: (768, 768)
model.t5.decoder.block.6.layer.1.EncDecAttention.v.weight: (768, 768)
model.t5.decoder.block.6.layer.1.EncDecAttention.o.weight: (768, 768)
model.t5.decoder.block.6.layer.1.layer_norm.weight: (768,)
model.t5.decoder.block.6.layer.2.DenseReluDense.wi.weight: (3072, 768)
model.t5.decoder.block.6.layer.2.DenseReluDense.wo.weight: (768, 3072)
model.t5.decoder.block.6.layer.2.layer_norm.weight: (768,)
model.t5.decoder.block.7.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.decoder.block.7.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.decoder.block.7.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.decoder.block.7.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.decoder.block.7.layer.0.layer_norm.weight: (768,)
model.t5.decoder.block.7.layer.1.EncDecAttention.q.weight: (768, 768)
model.t5.decoder.block.7.layer.1.EncDecAttention.k.weight: (768, 768)
model.t5.decoder.block.7.layer.1.EncDecAttention.v.weight: (768, 768)
model.t5.decoder.block.7.layer.1.EncDecAttention.o.weight: (768, 768)
model.t5.decoder.block.7.layer.1.layer_norm.weight: (768,)
model.t5.decoder.block.7.layer.2.DenseReluDense.wi.weight: (3072, 768)
model.t5.decoder.block.7.layer.2.DenseReluDense.wo.weight: (768, 3072)
model.t5.decoder.block.7.layer.2.layer_norm.weight: (768,)
model.t5.decoder.block.8.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.decoder.block.8.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.decoder.block.8.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.decoder.block.8.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.decoder.block.8.layer.0.layer_norm.weight: (768,)
model.t5.decoder.block.8.layer.1.EncDecAttention.q.weight: (768, 768)
model.t5.decoder.block.8.layer.1.EncDecAttention.k.weight: (768, 768)
model.t5.decoder.block.8.layer.1.EncDecAttention.v.weight: (768, 768)
model.t5.decoder.block.8.layer.1.EncDecAttention.o.weight: (768, 768)
model.t5.decoder.block.8.layer.1.layer_norm.weight: (768,)
model.t5.decoder.block.8.layer.2.DenseReluDense.wi.weight: (3072, 768)
model.t5.decoder.block.8.layer.2.DenseReluDense.wo.weight: (768, 3072)
model.t5.decoder.block.8.layer.2.layer_norm.weight: (768,)
model.t5.decoder.block.9.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.decoder.block.9.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.decoder.block.9.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.decoder.block.9.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.decoder.block.9.layer.0.layer_norm.weight: (768,)
model.t5.decoder.block.9.layer.1.EncDecAttention.q.weight: (768, 768)
model.t5.decoder.block.9.layer.1.EncDecAttention.k.weight: (768, 768)
model.t5.decoder.block.9.layer.1.EncDecAttention.v.weight: (768, 768)
model.t5.decoder.block.9.layer.1.EncDecAttention.o.weight: (768, 768)
model.t5.decoder.block.9.layer.1.layer_norm.weight: (768,)
model.t5.decoder.block.9.layer.2.DenseReluDense.wi.weight: (3072, 768)
model.t5.decoder.block.9.layer.2.DenseReluDense.wo.weight: (768, 3072)
model.t5.decoder.block.9.layer.2.layer_norm.weight: (768,)
model.t5.decoder.block.10.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.decoder.block.10.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.decoder.block.10.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.decoder.block.10.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.decoder.block.10.layer.0.layer_norm.weight: (768,)
model.t5.decoder.block.10.layer.1.EncDecAttention.q.weight: (768, 768)
model.t5.decoder.block.10.layer.1.EncDecAttention.k.weight: (768, 768)
model.t5.decoder.block.10.layer.1.EncDecAttention.v.weight: (768, 768)
model.t5.decoder.block.10.layer.1.EncDecAttention.o.weight: (768, 768)
model.t5.decoder.block.10.layer.1.layer_norm.weight: (768,)
model.t5.decoder.block.10.layer.2.DenseReluDense.wi.weight: (3072, 768)
model.t5.decoder.block.10.layer.2.DenseReluDense.wo.weight: (768, 3072)
model.t5.decoder.block.10.layer.2.layer_norm.weight: (768,)
model.t5.decoder.block.11.layer.0.SelfAttention.q.weight: (768, 768)
model.t5.decoder.block.11.layer.0.SelfAttention.k.weight: (768, 768)
model.t5.decoder.block.11.layer.0.SelfAttention.v.weight: (768, 768)
model.t5.decoder.block.11.layer.0.SelfAttention.o.weight: (768, 768)
model.t5.decoder.block.11.layer.0.layer_norm.weight: (768,)
model.t5.decoder.block.11.layer.1.EncDecAttention.q.weight: (768, 768)
model.t5.decoder.block.11.layer.1.EncDecAttention.k.weight: (768, 768)
model.t5.decoder.block.11.layer.1.EncDecAttention.v.weight: (768, 768)
model.t5.decoder.block.11.layer.1.EncDecAttention.o.weight: (768, 768)
model.t5.decoder.block.11.layer.1.layer_norm.weight: (768,)
model.t5.decoder.block.11.layer.2.DenseReluDense.wi.weight: (3072, 768)
model.t5.decoder.block.11.layer.2.DenseReluDense.wo.weight: (768, 3072)
model.t5.decoder.block.11.layer.2.layer_norm.weight: (768,)
model.t5.decoder.final_layer_norm.weight: (768,)
model.action_head.0.weight: (2048, 768)
model.action_head.0.bias: (2048,)
model.action_head.3.weight: (1, 2048)
model.action_head.3.bias: (1,)
model.lr_scorer.0.weight: (2048, 1536)
model.lr_scorer.0.bias: (2048,)
model.lr_scorer.3.weight: (1, 2048)
model.lr_scorer.3.bias: (1,)
model.rr_scorer.0.weight: (2048, 1556)
model.rr_scorer.0.bias: (2048,)
model.rr_scorer.3.weight: (1, 2048)
model.rr_scorer.3.bias: (1,)
model.emb_rr_distance.weight: (16, 20)
