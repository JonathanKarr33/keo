{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "476497ed-56d1-4549-b03d-0a80b5f0d412",
   "metadata": {},
   "source": [
    "### t5minimize_coref_breakdown\n",
    "\n",
    "This notebook takes the t5_minimize_coref.py from ASP/data and divides it into sections for testing\n",
    "\n",
    "The cnn_trial_coref folder in data is conll-12 data which is available for trial without license. I took the cnn data from the conll-12 trial folder and copied it three times (all three files in the cnn_trail_coref folder are really the same), and renamed them according to the expected naming convention. This is just to confirm what format asp expects, before we have the maintenance data in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f56a4b-de99-429a-a24a-cacabb657938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "import collections\n",
    "import logging\n",
    "from typing import Optional, Tuple, Any, Dict, Iterable, List\n",
    "\n",
    "import util\n",
    "import conll\n",
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea1015-d72e-4407-a10b-ea184ce1a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage:\n",
    "# python t5minimize_coref.py ontonotes_coref/ ontonotes_coref/\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__file__)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", model_max_length=4096)\n",
    "\n",
    "SPEAKER_START = '<speaker>'\n",
    "SPEAKER_END   = '</speaker>'\n",
    "MENTION_START = '<m>'\n",
    "MENTION_END   = '</m>'\n",
    "\n",
    "prefix_subtokens = tokenizer.tokenize(\"coreference resolution:\")\n",
    "prefix_len = len(prefix_subtokens)\n",
    "\n",
    "tokenizer.add_tokens(SPEAKER_START)\n",
    "tokenizer.add_tokens(SPEAKER_END)\n",
    "tokenizer.add_tokens(MENTION_START)\n",
    "tokenizer.add_tokens(MENTION_END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e433389-363e-4d39-a7b8-76757e034ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentState(object):\n",
    "    def __init__(self, key):\n",
    "        self.doc_key = key\n",
    "        self.sentence_end = []\n",
    "        self.token_end = []\n",
    "        self.tokens = []\n",
    "        self.subtokens = []\n",
    "        self.info = []\n",
    "        self.segments = []\n",
    "        self.subtoken_map = []\n",
    "        self.segment_subtoken_map = []\n",
    "        self.sentence_map = []\n",
    "        self.pronouns = []\n",
    "        self.clusters = collections.defaultdict(list)\n",
    "        self.coref_stacks = collections.defaultdict(list)\n",
    "        self.segment_info = []\n",
    "\n",
    "\n",
    "    def finalize(self):\n",
    "        # populate clusters\n",
    "        first_subtoken_index = -1\n",
    "        mention_to_seg_id = {}\n",
    "        for seg_idx, segment in enumerate(self.segment_info):\n",
    "            # keeping all segments\n",
    "            for i, tok_info in enumerate(segment):\n",
    "                first_subtoken_index += 1\n",
    "                coref = tok_info[-2] if tok_info is not None else '-'\n",
    "                if coref != \"-\":\n",
    "                    last_subtoken_index = first_subtoken_index + \\\n",
    "                        tok_info[-1] - 1\n",
    "                    for part in coref.split(\"|\"):\n",
    "                        if part[0] == \"(\":\n",
    "                            if part[-1] == \")\":\n",
    "                                cluster_id = int(part[1:-1])\n",
    "                                self.clusters[cluster_id].append((first_subtoken_index, last_subtoken_index))\n",
    "                                mention_to_seg_id[first_subtoken_index] = seg_idx\n",
    "                                mention_to_seg_id[last_subtoken_index+1] = seg_idx\n",
    "                            else:\n",
    "                                cluster_id = int(part[1:])\n",
    "                                self.coref_stacks[cluster_id].append(\n",
    "                                    first_subtoken_index)\n",
    "                        else:\n",
    "                            cluster_id = int(part[:-1])\n",
    "                            start = self.coref_stacks[cluster_id].pop()\n",
    "                            self.clusters[cluster_id].append((start, last_subtoken_index))\n",
    "                            mention_to_seg_id[start] = seg_idx\n",
    "                            mention_to_seg_id[last_subtoken_index+1] = seg_idx\n",
    "\n",
    "        # merge clusters\n",
    "        merged_clusters = []\n",
    "        for c1 in self.clusters.values():\n",
    "            existing = None\n",
    "            for m in c1:\n",
    "                for c2 in merged_clusters:\n",
    "                    if m in c2:\n",
    "                        existing = c2\n",
    "                        break\n",
    "                if existing is not None:\n",
    "                    break\n",
    "            if existing is not None:\n",
    "                logger.info(\"Merging clusters (shouldn't happen very often.)\")\n",
    "                existing.update(c1)\n",
    "            else:\n",
    "                merged_clusters.append(set(c1))\n",
    "\n",
    "        # merged_clusters: list of clusters\n",
    "        merged_clusters = [list(c) for c in merged_clusters]\n",
    "        cluster_indices = {\n",
    "            x: i for i in range(len(merged_clusters)) for x in merged_clusters[i]}\n",
    "\n",
    "        docs = []\n",
    "        num_words = len(util.flatten(self.segments))\n",
    "        num_segments = len(self.segments)\n",
    "\n",
    "        subtoken_map = self.segment_subtoken_map\n",
    "        assert num_words == len(util.flatten(self.segment_subtoken_map))\n",
    "\n",
    "        sentence_map = get_sentence_map(self.segments, self.sentence_end)\n",
    "        assert num_words == len(sentence_map), (num_words, len(sentence_map))\n",
    "\n",
    "        all_mentions = util.flatten(merged_clusters)\n",
    "        assert len(all_mentions) == len(set(all_mentions))\n",
    "        sentences = self.segments\n",
    "\n",
    "        # inserting <m> and </m> into target sequences for all mentions\n",
    "        target_sentences = m_star_target_sequences(\n",
    "            all_mentions, self.segments,\n",
    "            MENTION_START, MENTION_END,\n",
    "            mention_to_seg_id\n",
    "        )\n",
    "\n",
    "        # inserting mention indices to <\\m>\n",
    "        mention_indices = m_star_insert_info(\n",
    "            all_mentions, self.segments,\n",
    "            [ix for ix, x in enumerate(all_mentions)],\n",
    "            mention_to_seg_id\n",
    "        )\n",
    "        mention_indices = post_processing_mention_indices(mention_indices)\n",
    "        # inserting cluster indices to <\\m>\n",
    "        cluster_categories = m_star_insert_info(\n",
    "            all_mentions, self.segments,\n",
    "            [cluster_indices[x] for x in all_mentions],\n",
    "            mention_to_seg_id\n",
    "        )\n",
    "        for i in range(len(cluster_categories)):\n",
    "            cluster_categories[i] = [\n",
    "                x if type(x) != list else -1 for x in cluster_categories[i]]\n",
    "            assert len(cluster_categories[i]) == len(target_sentences[i]) == len(mention_indices[i])\n",
    "\n",
    "        clusters = [[] for _ in range(len(self.segments))]\n",
    "        for x in all_mentions:\n",
    "            clusters[mention_to_seg_id[x[0]]].append(x)\n",
    "\n",
    "        for i in range(num_segments):\n",
    "            docs.append({\n",
    "                \"doc_key\": f'{self.doc_key}_{i}',\n",
    "                \"sentence\": sentences[i],\n",
    "                \"target_sentence\": target_sentences[i],\n",
    "                \"mention_indice\": mention_indices[i],\n",
    "                \"cluster_category\": cluster_categories[i],\n",
    "                'sentence_map': sentence_map[i],\n",
    "                \"subtoken_map\": subtoken_map[i]\n",
    "            })\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98490bd8-ec6f-41ff-9de2-98253a8e1509",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple, Any, Dict, Iterable, List\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconll\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5Tokenizer\n",
      "File \u001b[0;32m/afs/crc.nd.edu/group/TAI/Users/kmealey2/git/2K-paper/coref/asp/ASP/util/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _batched_index_select\n",
      "File \u001b[0;32m/afs/crc.nd.edu/group/TAI/Users/kmealey2/git/2K-paper/coref/asp/ASP/util/func.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m makedirs\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m join\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyhocon\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "def post_processing_mention_indices(\n",
    "    mention_indices\n",
    "):\n",
    "    \"\"\"\n",
    "    Post-processing mention indices.\n",
    "    E.g.\n",
    "        [ q  <m>  a  b <\\m>  c <\\m> ]\n",
    "        [-1  -1  -1 -1   1  -1   1  ]\n",
    "    \"\"\"\n",
    "    tmp_mention_indices = []\n",
    "    for seg_i in range(len(mention_indices)):\n",
    "        tmp_mention_indices_seg_i = []\n",
    "        for j in range(len(mention_indices[seg_i])):\n",
    "            # reading from left to right\n",
    "            if type(mention_indices[seg_i][j]) != list:\n",
    "                # j is either 1. word or 2. closing bracket\n",
    "                tmp_mention_indices_seg_i.append(mention_indices[seg_i][j])\n",
    "                # putting the index of pairing opening bracket or -1\n",
    "                continue\n",
    "            else:\n",
    "                # j is opening bracket [*\n",
    "                tmp_mention_indices_seg_i.append(-1)\n",
    "                for k in range(j+1, len(mention_indices[seg_i])):\n",
    "                    if mention_indices[seg_i][k] in mention_indices[seg_i][j]:\n",
    "                        # the closing bracket k pairs with opening bracket j\n",
    "                        mention_indices[seg_i][k] = j\n",
    "        tmp_mention_indices.append(tmp_mention_indices_seg_i)\n",
    "    return tmp_mention_indices\n",
    "\n",
    "\n",
    "def m_star_insert_info(\n",
    "    mentions: List[Tuple[int, int]],\n",
    "    segments: List[List[str]], \n",
    "    m_infos: List[int], \n",
    "    mention_to_seg_id: Dict[int, int]\n",
    "):\n",
    "    \"\"\"\n",
    "        Get a sequence of information of the same length with the target sequence.\n",
    "        mentions: list of mentions, e.g. [(0, 0), (2, 3), (4, 4)] format: [start, end] (inclusive)\n",
    "        segments: list of segments, e.g. [['I', 'have', 'a', 'cat'], ['I', 'have', 'a', 'dog']]\n",
    "        m_infos: list of information to be inserted with each mention, \n",
    "                 e.g. cluster indices [0, 1, 2]\n",
    "        mention_to_seg_id: dict, mapping mention to its segment id\n",
    "    \"\"\"\n",
    "    m_startings, m_endings = zip(*mentions) if len(mentions) > 0 else ([], [])\n",
    "    # order preserving\n",
    "    sorted_pos = sorted(\n",
    "        [(x+1, -1, y) for x, y in zip(m_endings, m_infos)] +\n",
    "        [(x,  1, [y]) for x, y in zip(m_startings, m_infos)],\n",
    "        reverse=True # insert from right to left, so that the calculated positions are not changed\n",
    "    ) \n",
    "    # when inserting positions are the same, the closing bracket comes first\n",
    "    # which means that the closing bracket is inserted first\n",
    "    # and the opening bracket is inserted later\n",
    "\n",
    "    target_sequences = [\n",
    "        [-1 for x in range(len(segments[i]))] for i in range(len(segments))]\n",
    "    # offset of each segment\n",
    "    offsets = list(accumu([len(x) for x in segments]))\n",
    "\n",
    "    prev_loc, prev_token = -1, None\n",
    "    for x in sorted_pos:\n",
    "        seg_idx = mention_to_seg_id[x[0]]\n",
    "        offset = offsets[seg_idx]\n",
    "\n",
    "        if x[0] == prev_loc and (x[1] == prev_token == 1): # 1 for starting\n",
    "            # contracting left brackets\n",
    "            target_sequences[seg_idx][x[0]-offset].extend(x[2])\n",
    "        else:\n",
    "            target_sequences[seg_idx].insert(x[0]-offset, x[2])\n",
    "        prev_loc, prev_token = x[0], x[1]\n",
    "\n",
    "    return target_sequences\n",
    "\n",
    "\n",
    "def m_star_target_sequences(\n",
    "    mentions: List[Tuple[int, int]],\n",
    "    sequences: List[List[str]],\n",
    "    m_special_start: str, \n",
    "    m_special_end: str,\n",
    "    mention_to_seg_id: Dict[int, int]\n",
    "):\n",
    "    \"\"\"\n",
    "        Get a sequence of target sentences with <m> and <\\m> inserted.\n",
    "        mentions: list of mentions, e.g. [(0, 0), (2, 3), (4, 4)] format: [start, end] (inclusive)\n",
    "        sequences: list of sequences, e.g. [['I', 'have', 'a', 'cat'], ['I', 'have', 'a', 'dog']]\n",
    "        m_special_start: special token for starting bracket\n",
    "        m_special_end: special token for ending bracket\n",
    "        mention_to_seg_id: dict, mapping mention to its segment id\n",
    "    \"\"\"\n",
    "    m_startings, m_endings = zip(*mentions) if len(mentions) > 0 else ([], [])\n",
    "    sorted_pos = sorted(\n",
    "        [(x+1, -1, m_special_end)   for x in m_endings] +\n",
    "        [(x,    1, m_special_start) for x in m_startings],\n",
    "        reverse=True # insert from right to left, so that the calculated positions are not changed\n",
    "    )\n",
    "\n",
    "    target_sequences = copy.deepcopy(sequences)\n",
    "    # offset of each segment\n",
    "    offsets = list(accumu([len(x) for x in sequences]))\n",
    "\n",
    "    prev_loc, prev_token = -1, None\n",
    "    for x in sorted_pos:\n",
    "        seg_idx = mention_to_seg_id[x[0]]\n",
    "        offset = offsets[seg_idx]\n",
    "\n",
    "        if x[0] == prev_loc and (x[2] == prev_token == m_special_start):\n",
    "            # contracting left brackets to [*\n",
    "            pass # do nothing\n",
    "        else:\n",
    "            target_sequences[seg_idx].insert(x[0]-offset, x[2])\n",
    "        prev_loc, prev_token = x[0], x[2]\n",
    "\n",
    "    return target_sequences\n",
    "\n",
    "\n",
    "def normalize_word(word, language):\n",
    "    br_dict = {\"-LRB-\": \"(\", \"-RRB-\": \")\", \"-LSB-\": \"[\", \"-RSB-\": \"]\"}\n",
    "\n",
    "    if language == \"arabic\":\n",
    "        word = word[:word.find(\"#\")]\n",
    "\n",
    "    if word in br_dict:\n",
    "        word = br_dict[word]\n",
    "        return word\n",
    "    elif word == \"/.\" or word == \"/?\":\n",
    "        return word[1:]\n",
    "    elif word == \"''\" or word == \"``\": # <unk> otherwise\n",
    "        return \"\\\"\"\n",
    "    elif word == \"`\": # <unk> otherwise\n",
    "        return \"\\'\"\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "# first try to satisfy constraints1, and if not possible, constraints2.\n",
    "def split_into_segments(\n",
    "    document_state, max_segment_len, constraints1, constraints2\n",
    "):\n",
    "    current = 0\n",
    "    while current < len(document_state.subtokens):\n",
    "        end = min(current + max_segment_len - 1 - 1 - prefix_len,\n",
    "                  len(document_state.subtokens) - 1)\n",
    "\n",
    "        while end >= current and not constraints1[end]:\n",
    "            end -= 1\n",
    "\n",
    "        if end < current:\n",
    "            end = min(current + max_segment_len - 1 - 1 - prefix_len,\n",
    "                      len(document_state.subtokens) - 1)\n",
    "            while end >= current and not constraints2[end]:\n",
    "                end -= 1\n",
    "            if end < current:\n",
    "                raise Exception(\"Can't find valid segment\")\n",
    "\n",
    "        document_state.segments.append(\n",
    "            prefix_subtokens + document_state.subtokens[current:end+1] + ['</s>'])\n",
    "\n",
    "        subtoken_map = document_state.subtoken_map[current:end+1]\n",
    "        document_state.segment_subtoken_map.append(\n",
    "            [subtoken_map[0]] * prefix_len + subtoken_map + [subtoken_map[-1]])\n",
    "        document_state.segment_info.append(\n",
    "            [None] * prefix_len + document_state.info[current:end+1] + [None])\n",
    "        current = end + 1\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def get_sentence_map(segments, sentence_end):\n",
    "    current = 0\n",
    "    sent_map = []\n",
    "    sent_end_idx = 0\n",
    "    assert len(sentence_end) == sum([len(s) - 1 - prefix_len for s in segments])\n",
    "    for segment in segments:\n",
    "        sent_map.extend([current] * prefix_len)\n",
    "        for i in range(len(segment) - 1 - prefix_len):\n",
    "            sent_map.append(current)\n",
    "            current += int(sentence_end[sent_end_idx])\n",
    "            sent_end_idx += 1\n",
    "        sent_map.append(current)\n",
    "    return sent_map\n",
    "\n",
    "\n",
    "def get_document(\n",
    "    document_lines, tokenizer, language, segment_len\n",
    "):\n",
    "    document_state = DocumentState(document_lines[0])\n",
    "    word_idx = -1\n",
    "\n",
    "    current_speaker = None\n",
    "    after_hyphen = False\n",
    "    doc_lines = document_lines[1]\n",
    "\n",
    "    for line in doc_lines:\n",
    "        row = line.split()\n",
    "        sentence_end = len(row) == 0\n",
    "        if not sentence_end:\n",
    "            assert len(row) >= 12\n",
    "            speaker_orthography = row[9].replace(\"_\", \" \").replace(\"#\", \" \").strip()\n",
    "            if current_speaker is None or current_speaker != speaker_orthography:\n",
    "                # insert speaker\n",
    "                word_idx += 1\n",
    "                current_speaker = speaker_orthography\n",
    "                speaker_text = tokenizer.tokenize(current_speaker)\n",
    "                document_state.tokens.append(current_speaker)\n",
    "\n",
    "                for sidx, subtoken in enumerate([SPEAKER_START] + speaker_text + [SPEAKER_END]):\n",
    "                    document_state.subtokens.append(subtoken)\n",
    "                    info = None\n",
    "                    document_state.info.append(info)\n",
    "                    document_state.sentence_end.append(False)\n",
    "                    document_state.subtoken_map.append(word_idx)\n",
    "\n",
    "            word_idx += 1\n",
    "            word = normalize_word(row[3], language)\n",
    "\n",
    "            if is_punctuation(word):\n",
    "                subtokens = tokenizer.tokenize(word)[1:]  # skipping '_'\n",
    "            elif after_hyphen:\n",
    "                subtokens = tokenizer.tokenize(\"-\"+word)  # skipping '_'\n",
    "                if subtokens[1] == \"-\":\n",
    "                    subtokens = subtokens[2:]\n",
    "                else:\n",
    "                    subtokens = subtokens[1:]\n",
    "                after_hyphen = False\n",
    "            else:\n",
    "                subtokens = tokenizer.tokenize(word)\n",
    "\n",
    "            if row[4] == \"HYPH\":\n",
    "                after_hyphen = True\n",
    "\n",
    "            document_state.tokens.append(word)\n",
    "            document_state.token_end += [False] * (len(subtokens) - 1) + [True]\n",
    "\n",
    "            for sidx, subtoken in enumerate(subtokens):\n",
    "                document_state.subtokens.append(subtoken)\n",
    "                info = None if sidx != 0 else (row + [len(subtokens)])\n",
    "                document_state.info.append(info)\n",
    "                document_state.sentence_end.append(False)\n",
    "                document_state.subtoken_map.append(word_idx)\n",
    "        else:\n",
    "            document_state.sentence_end[-1] = True\n",
    "\n",
    "    constraints1 = (\n",
    "        document_state.sentence_end\n",
    "        if language != \"arabic\"\n",
    "        else document_state.token_end\n",
    "    )\n",
    "    split_into_segments(\n",
    "        document_state, segment_len, constraints1, document_state.token_end\n",
    "    )\n",
    "\n",
    "    stats[f\"max_seg_len\"] = max(\n",
    "        stats[\"max_seg_len\"], max([len(s) for s in document_state.segments])\n",
    "    )\n",
    "    stats[f\"max_num_seg\"] = max(\n",
    "        len(document_state.segments), stats[f\"max_num_seg\"]\n",
    "    )\n",
    "    document = document_state.finalize()\n",
    "    return document\n",
    "\n",
    "\n",
    "def is_punctuation(c):\n",
    "    if (\n",
    "        c in {\".\", \",\", \"?\", \"!\", \";\", \n",
    "        \":\", \"'s\", \"'m\", \"'ve\", \"n't\", \"'ll\",\n",
    "        \")\", \"]\", \"}\", \"-\"}\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_special(c):\n",
    "    if (\n",
    "        c in {\"<pad>\", \"</s>\", \"<unk>\"}\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def accumu(lis):\n",
    "    total = 0\n",
    "    for x in lis:\n",
    "        yield total\n",
    "        total += x\n",
    "\n",
    "def minimize_partition(\n",
    "    name, language, extension, stats, tokenizer, seg_len, input_dir, output_dir\n",
    "):\n",
    "    input_path = \"{}/{}.{}.{}\".format(input_dir, name, language, extension)\n",
    "    output_path = \"{}/{}.t5-small.{}.{}.jsonlines\".format(output_dir, name, language, seg_len)\n",
    "\n",
    "    count = 0\n",
    "    logger.info(\"Minimizing {}\".format(input_path))\n",
    "    documents = []\n",
    "    with open(input_path, \"r\") as input_file:\n",
    "        for line in input_file.readlines():\n",
    "            begin_document_match = re.match(conll.BEGIN_DOCUMENT_REGEX, line)\n",
    "            if begin_document_match:\n",
    "                doc_key = conll.get_doc_key(\n",
    "                    begin_document_match.group(1), begin_document_match.group(2)\n",
    "                )\n",
    "                documents.append((doc_key, []))\n",
    "            elif line.startswith(\"#end document\"):\n",
    "                continue\n",
    "            else:\n",
    "                documents[-1][1].append(line)\n",
    "\n",
    "    datasets, max_target_len = [], 0\n",
    "    max_input_len = 0\n",
    "    for document_lines in documents:\n",
    "        max_input_len = max(max_input_len, len([x for x in document_lines[1] if len(x) > 2]))\n",
    "        document = get_document(document_lines, tokenizer, language, seg_len)\n",
    "        for doc in document:\n",
    "            max_target_len = max([max_target_len] + [len(doc['cluster_category'])])\n",
    "            datasets.append(doc)\n",
    "            count += 1\n",
    "    json.dump(datasets, open(output_path, \"w\"))\n",
    "    logger.info(f\"Maximum input sequence length: {max_input_len}, Maximum target sequence length: {max_target_len}\")\n",
    "    logger.info(\"Wrote {} documents to {}\".format(count, output_path))\n",
    "\n",
    "\n",
    "def minimize_language(language, stats, seg_len, input_dir, output_dir):\n",
    "    minimize_partition(\"dev\", language, \"v4_gold_conll\", stats,\n",
    "                       tokenizer, seg_len, input_dir, output_dir)\n",
    "    minimize_partition(\"train\", language, \"v4_gold_conll\", stats,\n",
    "                       tokenizer, seg_len, input_dir, output_dir)\n",
    "    minimize_partition(\"test\", language, \"v4_gold_conll\", stats,\n",
    "                       tokenizer, seg_len, input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb66e7-94df-4e78-a583-d0019c6fc015",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = sys.argv[1]\n",
    "output_dir = sys.argv[2]\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "for seg_len in [4096, 2048]:\n",
    "    stats = collections.defaultdict(int)\n",
    "    minimize_language(\"english\", stats, seg_len, input_dir, output_dir)\n",
    "\n",
    "    logger.info(\"Dataset stats:\")\n",
    "    for k, v in stats.items():\n",
    "        logger.info(\"{} = {}\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025e193-780d-49d7-8378-32a411f9e092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t5min",
   "language": "python",
   "name": "t5min"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
